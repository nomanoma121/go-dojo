# Day 58: Prometheus Histogram Metrics

## ğŸ¯ æœ¬æ—¥ã®ç›®æ¨™ (Today's Goal)

Prometheusãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®Ÿè£…ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†å¸ƒã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’è¡Œã†é«˜åº¦ãªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—ã€ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã®è¨­å®šã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†ææ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹ã€‚

## ğŸ“– è§£èª¬ (Explanation)

### ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨ã¯

ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¯è¦³æ¸¬å€¤ã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒã‚±ãƒƒãƒˆï¼ˆåŒºé–“ï¼‰ã«åˆ†é¡ã—ã¦ã€å€¤ã®åˆ†å¸ƒã‚’æ¸¬å®šã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‹ã§ã™ã€‚ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãªã©ã€å€¤ã®ç¯„å›²ãŒåºƒãåˆ†å¸ƒã®å½¢çŠ¶ãŒé‡è¦ãªæŒ‡æ¨™ã®ç›£è¦–ã«é©ã—ã¦ã„ã¾ã™ã€‚

### ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®æ§‹é€ 

Prometheusãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¯3ã¤ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ãƒªãƒ¼ã‚ºã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ï¼š

```
# ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆ
http_request_duration_seconds_bucket{le="0.1"} 850
http_request_duration_seconds_bucket{le="0.5"} 1200  
http_request_duration_seconds_bucket{le="1.0"} 1450
http_request_duration_seconds_bucket{le="+Inf"} 1500

# å…¨è¦³æ¸¬å€¤ã®åˆè¨ˆ
http_request_duration_seconds_sum 425.3

# è¦³æ¸¬å›æ•°ã®ç·æ•°
http_request_duration_seconds_count 1500
```

### ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®åˆ©ç‚¹ã¨ç‰¹å¾´

#### 1. ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—

ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‹ã‚‰æ§˜ã€…ãªãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚’è¨ˆç®—ã§ãã¾ã™ï¼š

```promql
# 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# 50ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆä¸­å¤®å€¤ï¼‰
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))

# 99.9ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
histogram_quantile(0.999, rate(http_request_duration_seconds_bucket[5m]))
```

#### 2. é›†ç´„å¯èƒ½æ€§

è¤‡æ•°ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’é›†ç´„ã§ãã¾ã™ï¼š

```promql
# è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
histogram_quantile(0.95, 
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)

# ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
histogram_quantile(0.95,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
)
```

#### 3. SLI/SLO ç›£è¦–

Service Level Indicatorsï¼ˆSLIï¼‰ã¨Service Level Objectivesï¼ˆSLOï¼‰ã®ç›£è¦–ï¼š

```promql
# 95%ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒ100msä»¥å†…ï¼ˆSLIï¼‰
(
  sum(rate(http_request_duration_seconds_bucket{le="0.1"}[5m])) 
  / 
  sum(rate(http_request_duration_seconds_count[5m]))
) * 100

# ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆæ¶ˆè²»ç‡
1 - (
  sum(rate(http_request_duration_seconds_bucket{le="0.1"}[5m])) 
  / 
  sum(rate(http_request_duration_seconds_count[5m]))
)
```

### ãƒã‚±ãƒƒãƒˆè¨­è¨ˆã®è€ƒæ…®äº‹é …

#### 1. é©åˆ‡ãªãƒã‚±ãƒƒãƒˆå¢ƒç•Œ

```go
// Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼ˆãƒŸãƒªç§’ï¼‰
webBuckets := []float64{
    0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0,
}

// APIå‡¦ç†æ™‚é–“ç”¨ï¼ˆç§’ï¼‰
apiBuckets := []float64{
    0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0, 120.0,
}

// ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç”¨ï¼ˆãƒã‚¤ãƒˆï¼‰
fileSizeBuckets := []float64{
    1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216,
}

// ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªç”¨ï¼ˆãƒŸãƒªç§’ï¼‰
dbBuckets := []float64{
    0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0,
}
```

#### 2. æŒ‡æ•°çš„ãƒã‚±ãƒƒãƒˆç”Ÿæˆ

```go
import "github.com/prometheus/client_golang/prometheus"

// æŒ‡æ•°çš„ãƒã‚±ãƒƒãƒˆç”Ÿæˆ
// Start=0.1, Factor=2, Count=10
// çµæœ: [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2]
buckets := prometheus.ExponentialBuckets(0.1, 2, 10)

// ç·šå½¢ãƒã‚±ãƒƒãƒˆç”Ÿæˆ  
// Start=0, Width=10, Count=20
// çµæœ: [0, 10, 20, 30, ..., 190]
linearBuckets := prometheus.LinearBuckets(0, 10, 20)
```

### é«˜åº¦ãªãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å®Ÿè£…

#### 1. è¤‡æ•°æ¬¡å…ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 

```go
type RequestLatencyTracker struct {
    histogram *prometheus.HistogramVec
}

func NewRequestLatencyTracker() *RequestLatencyTracker {
    return &RequestLatencyTracker{
        histogram: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_request_duration_seconds",
                Help: "Time spent on HTTP requests",
                Buckets: []float64{
                    0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0,
                },
            },
            []string{"method", "endpoint", "status_class"}, // 2xx, 4xx, 5xx
        ),
    }
}

func (t *RequestLatencyTracker) TrackRequest(method, endpoint string, statusCode int, duration time.Duration) {
    statusClass := fmt.Sprintf("%dxx", statusCode/100)
    t.histogram.WithLabelValues(method, endpoint, statusClass).Observe(duration.Seconds())
}
```

#### 2. è‡ªå‹•çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

```go
type HistogramMiddleware struct {
    latencyTracker    *RequestLatencyTracker
    sizeTracker       *prometheus.HistogramVec
    concurrencyGauge  *prometheus.GaugeVec
}

func NewHistogramMiddleware() *HistogramMiddleware {
    return &HistogramMiddleware{
        latencyTracker: NewRequestLatencyTracker(),
        sizeTracker: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_request_size_bytes",
                Help: "Size of HTTP requests in bytes",
                Buckets: prometheus.ExponentialBuckets(64, 4, 8), // 64B to 1GB
            },
            []string{"method", "endpoint"},
        ),
        concurrencyGauge: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "http_requests_in_flight",
                Help: "Number of HTTP requests currently being processed",
            },
            []string{"endpoint"},
        ),
    }
}

func (m *HistogramMiddleware) Middleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        endpoint := r.URL.Path
        
        // åŒæ™‚å®Ÿè¡Œæ•°ã‚’è¿½è·¡
        m.concurrencyGauge.WithLabelValues(endpoint).Inc()
        defer m.concurrencyGauge.WithLabelValues(endpoint).Dec()
        
        // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’è¨˜éŒ²
        if r.ContentLength > 0 {
            m.sizeTracker.WithLabelValues(r.Method, endpoint).Observe(float64(r.ContentLength))
        }
        
        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’ãƒ©ãƒƒãƒ—
        ww := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
        
        // æ¬¡ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’å®Ÿè¡Œ
        next.ServeHTTP(ww, r)
        
        // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨˜éŒ²
        duration := time.Since(start)
        m.latencyTracker.TrackRequest(r.Method, endpoint, ww.statusCode, duration)
    })
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

#### 1. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†æå™¨

```go
type LatencyAnalyzer struct {
    tracker *RequestLatencyTracker
}

func (a *LatencyAnalyzer) AnalyzePerformance() PerformanceReport {
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’å–å¾—
    metricFamilies, err := prometheus.DefaultGatherer.Gather()
    if err != nil {
        return PerformanceReport{}
    }
    
    report := PerformanceReport{
        Timestamp: time.Now(),
        Endpoints: make([]EndpointPerformance, 0),
    }
    
    for _, mf := range metricFamilies {
        if mf.GetName() == "http_request_duration_seconds" {
            report.Endpoints = a.analyzeHistogramMetrics(mf)
        }
    }
    
    return report
}

func (a *LatencyAnalyzer) analyzeHistogramMetrics(mf *dto.MetricFamily) []EndpointPerformance {
    endpointStats := make(map[string]*EndpointPerformance)
    
    for _, metric := range mf.GetMetric() {
        labels := make(map[string]string)
        for _, label := range metric.GetLabel() {
            labels[label.GetName()] = label.GetValue()
        }
        
        endpoint := labels["endpoint"]
        if endpoint == "" {
            continue
        }
        
        if _, exists := endpointStats[endpoint]; !exists {
            endpointStats[endpoint] = &EndpointPerformance{
                Endpoint: endpoint,
                Buckets:  make([]BucketData, 0),
            }
        }
        
        hist := metric.GetHistogram()
        endpointStats[endpoint].Count = hist.GetSampleCount()
        endpointStats[endpoint].Sum = hist.GetSampleSum()
        
        if endpointStats[endpoint].Count > 0 {
            endpointStats[endpoint].Average = endpointStats[endpoint].Sum / float64(endpointStats[endpoint].Count)
        }
        
        // ãƒã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        for _, bucket := range hist.GetBucket() {
            endpointStats[endpoint].Buckets = append(endpointStats[endpoint].Buckets, BucketData{
                UpperBound: bucket.GetUpperBound(),
                Count:      bucket.GetCumulativeCount(),
            })
        }
        
        // ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—
        endpointStats[endpoint].P50 = a.calculatePercentile(endpointStats[endpoint].Buckets, 0.5, endpointStats[endpoint].Count)
        endpointStats[endpoint].P95 = a.calculatePercentile(endpointStats[endpoint].Buckets, 0.95, endpointStats[endpoint].Count)
        endpointStats[endpoint].P99 = a.calculatePercentile(endpointStats[endpoint].Buckets, 0.99, endpointStats[endpoint].Count)
    }
    
    // ãƒãƒƒãƒ—ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã«å¤‰æ›
    result := make([]EndpointPerformance, 0, len(endpointStats))
    for _, ep := range endpointStats {
        result = append(result, *ep)
    }
    
    return result
}

func (a *LatencyAnalyzer) calculatePercentile(buckets []BucketData, percentile float64, totalCount uint64) float64 {
    if len(buckets) == 0 || totalCount == 0 {
        return 0
    }
    
    targetCount := float64(totalCount) * percentile
    var prevBound float64 = 0
    var prevCount uint64 = 0
    
    for _, bucket := range buckets {
        if float64(bucket.Count) >= targetCount {
            // ç·šå½¢è£œé–“ã§ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã‚’è¨ˆç®—
            if bucket.Count == prevCount {
                return prevBound
            }
            
            ratio := (targetCount - float64(prevCount)) / float64(bucket.Count-prevCount)
            return prevBound + ratio*(bucket.UpperBound-prevBound)
        }
        
        prevBound = bucket.UpperBound
        prevCount = bucket.Count
    }
    
    return buckets[len(buckets)-1].UpperBound
}

type PerformanceReport struct {
    Timestamp time.Time             `json:"timestamp"`
    Endpoints []EndpointPerformance `json:"endpoints"`
}

type EndpointPerformance struct {
    Endpoint string       `json:"endpoint"`
    Count    uint64       `json:"count"`
    Sum      float64      `json:"sum"`
    Average  float64      `json:"average"`
    P50      float64      `json:"p50"`
    P95      float64      `json:"p95"`
    P99      float64      `json:"p99"`
    Buckets  []BucketData `json:"buckets"`
}

type BucketData struct {
    UpperBound float64 `json:"upper_bound"`
    Count      uint64  `json:"count"`
}
```

#### 2. ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 

```go
type AlertingSystem struct {
    tracker    *RequestLatencyTracker
    thresholds map[string]LatencyThreshold
    alertCh    chan Alert
}

type LatencyThreshold struct {
    P95Threshold float64 `json:"p95_threshold"`
    P99Threshold float64 `json:"p99_threshold"`
    ErrorRate    float64 `json:"error_rate"`
}

type Alert struct {
    Type         string    `json:"type"`
    Endpoint     string    `json:"endpoint"`
    Message      string    `json:"message"`
    Severity     string    `json:"severity"`
    Timestamp    time.Time `json:"timestamp"`
    CurrentValue float64   `json:"current_value"`
    Threshold    float64   `json:"threshold"`
}

func NewAlertingSystem(tracker *RequestLatencyTracker) *AlertingSystem {
    return &AlertingSystem{
        tracker:    tracker,
        thresholds: make(map[string]LatencyThreshold),
        alertCh:    make(chan Alert, 100),
    }
}

func (as *AlertingSystem) SetThreshold(endpoint string, threshold LatencyThreshold) {
    as.thresholds[endpoint] = threshold
}

func (as *AlertingSystem) StartMonitoring(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            as.checkThresholds()
        }
    }
}

func (as *AlertingSystem) checkThresholds() {
    analyzer := &LatencyAnalyzer{tracker: as.tracker}
    report := analyzer.AnalyzePerformance()
    
    for _, ep := range report.Endpoints {
        if threshold, exists := as.thresholds[ep.Endpoint]; exists {
            // P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒã‚§ãƒƒã‚¯
            if ep.P95 > threshold.P95Threshold {
                alert := Alert{
                    Type:         "latency_p95",
                    Endpoint:     ep.Endpoint,
                    Message:      "P95 latency exceeds threshold",
                    Severity:     "warning",
                    Timestamp:    time.Now(),
                    CurrentValue: ep.P95,
                    Threshold:    threshold.P95Threshold,
                }
                
                select {
                case as.alertCh <- alert:
                    log.Printf("Alert: P95 latency for %s is %.3fs (threshold: %.3fs)", 
                        ep.Endpoint, ep.P95, threshold.P95Threshold)
                default:
                    log.Printf("Alert channel full, dropping alert")
                }
            }
            
            // P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒã‚§ãƒƒã‚¯
            if ep.P99 > threshold.P99Threshold {
                alert := Alert{
                    Type:         "latency_p99",
                    Endpoint:     ep.Endpoint,
                    Message:      "P99 latency exceeds threshold",
                    Severity:     "critical",
                    Timestamp:    time.Now(),
                    CurrentValue: ep.P99,
                    Threshold:    threshold.P99Threshold,
                }
                
                select {
                case as.alertCh <- alert:
                    log.Printf("CRITICAL: P99 latency for %s is %.3fs (threshold: %.3fs)", 
                        ep.Endpoint, ep.P99, threshold.P99Threshold)
                default:
                    log.Printf("Alert channel full, dropping critical alert")
                }
            }
        }
    }
}

func (as *AlertingSystem) GetAlerts() <-chan Alert {
    return as.alertCh
}
```

### Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å¯¾å¿œ

#### 1. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­è¨ˆ

```go
type GrafanaMetrics struct {
    // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    RequestDuration *prometheus.HistogramVec
    
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼  
    RequestsTotal *prometheus.CounterVec
    
    // ã‚¨ãƒ©ãƒ¼ç‡ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    ErrorsTotal *prometheus.CounterVec
    
    // è¿½åŠ ã®åˆ†æç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    SlowRequests    *prometheus.CounterVec  // é–¾å€¤ã‚’è¶…ãˆã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    RequestSize     *prometheus.HistogramVec // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºåˆ†å¸ƒ
    ResponseSize    *prometheus.HistogramVec // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚ºåˆ†å¸ƒ
}

func NewGrafanaMetrics() *GrafanaMetrics {
    return &GrafanaMetrics{
        RequestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_request_duration_seconds",
                Help: "HTTP request duration in seconds",
                Buckets: []float64{0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0},
            },
            []string{"method", "endpoint", "status_class"},
        ),
        RequestsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "http_requests_total",
                Help: "Total number of HTTP requests",
            },
            []string{"method", "endpoint", "status"},
        ),
        ErrorsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "http_errors_total",
                Help: "Total number of HTTP errors",
            },
            []string{"method", "endpoint", "status"},
        ),
        SlowRequests: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "http_slow_requests_total",
                Help: "Total number of slow HTTP requests",
            },
            []string{"method", "endpoint", "threshold"},
        ),
        RequestSize: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_request_size_bytes",
                Help: "HTTP request size in bytes",
                Buckets: prometheus.ExponentialBuckets(64, 4, 8),
            },
            []string{"method", "endpoint"},
        ),
        ResponseSize: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_response_size_bytes", 
                Help: "HTTP response size in bytes",
                Buckets: prometheus.ExponentialBuckets(64, 4, 8),
            },
            []string{"method", "endpoint", "status_class"},
        ),
    }
}
```

#### 2. PromQL ã‚¯ã‚¨ãƒªä¾‹

```promql
# 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆ5åˆ†é–“ï¼‰
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåˆ¥ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
histogram_quantile(0.95, 
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint)
)

# SLI: 100msä»¥å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰²åˆ
sum(rate(http_request_duration_seconds_bucket{le="0.1"}[5m])) 
/ 
sum(rate(http_request_duration_seconds_count[5m]))

# ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆRPSï¼‰
sum(rate(http_requests_total[5m]))

# ã‚¨ãƒ©ãƒ¼ç‡
sum(rate(http_errors_total[5m])) 
/ 
sum(rate(http_requests_total[5m]))

# å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
rate(http_request_duration_seconds_sum[5m]) 
/ 
rate(http_request_duration_seconds_count[5m])
```

## ğŸ“ èª²é¡Œ (The Problem)

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤Prometheusãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

### 1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```go
type RequestLatencyTracker struct {
    histogram *prometheus.HistogramVec
}
```

### 2. å¿…è¦ãªæ©Ÿèƒ½

- **å¤šæ¬¡å…ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹**: method, endpoint, status ã«ã‚ˆã‚‹åˆ†é¡
- **é©åˆ‡ãªãƒã‚±ãƒƒãƒˆè¨­è¨ˆ**: Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é©ã—ãŸãƒã‚±ãƒƒãƒˆå¢ƒç•Œ
- **è‡ªå‹•åé›†ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢**: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**: ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- **ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ **: é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½

### 3. ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½

- ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- P50/P95/P99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã®è¨ˆç®—
- å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ç®—å‡º
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆåˆ†æ

### 4. ç›£è¦–æ©Ÿèƒ½

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ
- é–¾å€¤è¶…éã®æ¤œå‡º
- ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã®ç®¡ç†

## âœ… æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹• (Expected Behavior)

å®Ÿè£…ãŒæ­£ã—ãå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ†ã‚¹ãƒˆçµæœãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

```bash
$ go test -v
=== RUN   TestHistogram_BasicObservation
    main_test.go:45: Histogram observation recorded correctly
--- PASS: TestHistogram_BasicObservation (0.01s)

=== RUN   TestHistogram_MultipleObservations
    main_test.go:65: Multiple observations recorded correctly
    main_test.go:68: Bucket distribution is accurate
--- PASS: TestHistogram_MultipleObservations (0.01s)

=== RUN   TestPercentileCalculation
    main_test.go:85: P50: 0.250s, P95: 0.950s, P99: 0.990s
--- PASS: TestPercentileCalculation (0.02s)

=== RUN   TestAlertingSystem
    main_test.go:105: Alert triggered for P95 threshold violation
--- PASS: TestAlertingSystem (0.05s)

PASS
ok      day58-prometheus-histogram   0.156s
```

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡ºåŠ›ä¾‹

```
# HELP http_request_duration_seconds Time spent on HTTP requests
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.001"} 45
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.005"} 250
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.01"} 500
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.025"} 800
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.05"} 950
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="0.1"} 990
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",status_class="2xx",le="+Inf"} 1000
http_request_duration_seconds_sum{method="GET",endpoint="/api/users",status_class="2xx"} 15.5
http_request_duration_seconds_count{method="GET",endpoint="/api/users",status_class="2xx"} 1000
```

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ (Hints)

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã®ãƒ’ãƒ³ãƒˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

### åŸºæœ¬çš„ãªãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å®Ÿè£…

```go
func NewRequestLatencyTracker() *RequestLatencyTracker {
    return &RequestLatencyTracker{
        histogram: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "http_request_duration_seconds",
                Help: "Time spent on HTTP requests",
                Buckets: []float64{
                    0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0,
                },
            },
            []string{"method", "endpoint", "status_class"},
        ),
    }
}
```

### ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—

```go
func calculatePercentile(buckets []BucketData, percentile float64, totalCount uint64) float64 {
    if len(buckets) == 0 || totalCount == 0 {
        return 0
    }
    
    targetCount := float64(totalCount) * percentile
    var prevBound float64 = 0
    var prevCount uint64 = 0
    
    for _, bucket := range buckets {
        if float64(bucket.Count) >= targetCount {
            if bucket.Count == prevCount {
                return prevBound
            }
            
            ratio := (targetCount - float64(prevCount)) / float64(bucket.Count-prevCount)
            return prevBound + ratio*(bucket.UpperBound-prevBound)
        }
        
        prevBound = bucket.UpperBound
        prevCount = bucket.Count
    }
    
    return buckets[len(buckets)-1].UpperBound
}
```

### ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢å®Ÿè£…

```go
func (t *RequestLatencyTracker) Middleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        ww := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
        
        next.ServeHTTP(ww, r)
        
        duration := time.Since(start)
        statusClass := fmt.Sprintf("%dxx", ww.statusCode/100)
        
        t.histogram.WithLabelValues(r.Method, r.URL.Path, statusClass).Observe(duration.Seconds())
    })
}
```

## ğŸš€ ç™ºå±•èª²é¡Œ (Advanced Challenges)

åŸºæœ¬å®Ÿè£…ãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ç™ºå±•çš„ãªæ©Ÿèƒ½ã«ã‚‚ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¦ã¿ã¦ãã ã•ã„ï¼š

1. **å‹•çš„ãƒã‚±ãƒƒãƒˆèª¿æ•´**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ããƒã‚±ãƒƒãƒˆå¢ƒç•Œã®æœ€é©åŒ–
2. **è¤‡æ•°æ™‚é–“è»¸åˆ†æ**: çŸ­æœŸ/ä¸­æœŸ/é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¯”è¼ƒ
3. **ç•°å¸¸æ¤œçŸ¥**: çµ±è¨ˆçš„æ‰‹æ³•ã«ã‚ˆã‚‹ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
4. **å®¹é‡è¨ˆç”»**: æˆé•·äºˆæ¸¬ã¨ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°
5. **ã‚³ã‚¹ãƒˆåˆ†æ**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã‚³ã‚¹ãƒˆã®è©³ç´°åˆ†æ

Prometheusãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å®Ÿè£…ã‚’é€šã˜ã¦ã€é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã—ã‚‡ã†ï¼
# Day 36: DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…

## ğŸ¯ æœ¬æ—¥ã®ç›®æ¨™

N+1å•é¡Œã‚’æ ¹æœ¬çš„ã«è§£æ±ºã™ã‚‹DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ·±ãç†è§£ã—ã€é«˜æ€§èƒ½ãªãƒãƒƒãƒå‡¦ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚å®Ÿç”¨çš„ãªã‚·ãƒŠãƒªã‚ªã‚’é€šã˜ã¦ã€å¤§è¦æ¨¡ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§æ±‚ã‚ã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹ã€‚

## ğŸ“– è§£èª¬

### DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã¯

```go
// ã€DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¦æ€§ã€‘å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã§ã®ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ
// âŒ å•é¡Œä¾‹ï¼šN+1å•é¡Œã«ã‚ˆã‚‹å£Šæ»…çš„æ€§èƒ½åŠ£åŒ–ã¨ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
func catastrophicNPlusOneProblem() {
    // ğŸš¨ ç½å®³ä¾‹ï¼šDataLoaderæœªä½¿ç”¨ã«ã‚ˆã‚‹æ·±åˆ»ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã¨ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
    
    // âŒ æœ€æ‚ªã®å®Ÿè£…ï¼šN+1å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
    func GetUserTimelineBadly(userID int) (*Timeline, error) {
        // 1. ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—ï¼ˆ1å›ç›®ã®ã‚¯ã‚¨ãƒªï¼‰
        following, err := getFollowingUsers(userID) // 10,000äººãƒ•ã‚©ãƒ­ãƒ¼ä¸­
        if err != nil {
            return nil, err
        }
        
        var timelineItems []*TimelineItem
        
        // âŒ å„ãƒ•ã‚©ãƒ­ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’å€‹åˆ¥å–å¾—ï¼ˆNå›ã®ã‚¯ã‚¨ãƒªï¼‰
        for _, followedUser := range following { // 10,000å›ãƒ«ãƒ¼ãƒ—
            // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¯å›ã‚¢ã‚¯ã‚»ã‚¹ - ã“ã‚ŒãŒè‡´å‘½çš„
            userPosts, err := getPostsByUserID(followedUser.ID)
            if err != nil {
                continue // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å‡¦ç†ç¶™ç¶š
            }
            
            // å„æŠ•ç¨¿ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            for _, post := range userPosts {
                // ã•ã‚‰ã«æŠ•ç¨¿ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆã„ã„ã­æ•°ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãªã©ï¼‰
                postDetails, err := getPostDetails(post.ID) // ã•ã‚‰ã«Nå›
                if err != nil {
                    continue
                }
                
                // ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                comments, err := getCommentsByPostID(post.ID) // ã•ã‚‰ã«Nå›
                if err != nil {
                    continue
                }
                
                // å„ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                for _, comment := range comments {
                    commentUser, err := getUserByID(comment.UserID) // ã•ã‚‰ã«Nå›
                    if err != nil {
                        continue
                    }
                    comment.User = commentUser
                }
                
                // ã„ã„ã­ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                likes, err := getLikesByPostID(post.ID) // ã•ã‚‰ã«Nå›
                if err != nil {
                    continue
                }
                
                for _, like := range likes {
                    likeUser, err := getUserByID(like.UserID) // ã•ã‚‰ã«Nå›
                    if err != nil {
                        continue
                    }
                    like.User = likeUser
                }
                
                timelineItems = append(timelineItems, &TimelineItem{
                    Post:     post,
                    Details:  postDetails,
                    Comments: comments,
                    Likes:    likes,
                })
            }
        }
        
        // ã€ç½å®³çš„çµæœã€‘
        // - åˆæœŸã‚¯ã‚¨ãƒª: 1å›ï¼ˆãƒ•ã‚©ãƒ­ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—ï¼‰
        // - æŠ•ç¨¿å–å¾—: 10,000å›
        // - æŠ•ç¨¿è©³ç´°: 50,000å›ï¼ˆå„ãƒ¦ãƒ¼ã‚¶ãƒ¼5æŠ•ç¨¿ï¼‰
        // - ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—: 50,000å›
        // - ã‚³ãƒ¡ãƒ³ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼: 500,000å›ï¼ˆ1æŠ•ç¨¿10ã‚³ãƒ¡ãƒ³ãƒˆæƒ³å®šï¼‰
        // - ã„ã„ã­å–å¾—: 50,000å›
        // - ã„ã„ã­ãƒ¦ãƒ¼ã‚¶ãƒ¼: 1,000,000å›ï¼ˆ1æŠ•ç¨¿20ã„ã„ã­æƒ³å®šï¼‰
        // åˆè¨ˆ: 1,660,001å›ã®ã‚¯ã‚¨ãƒªï¼ï¼ï¼
        
        return &Timeline{Items: timelineItems}, nil
    }
    
    // âŒ ECã‚µã‚¤ãƒˆã§ã®å•†å“ä¸€è¦§è¡¨ç¤ºã§ã®ç½å®³
    func GetProductCatalogBadly() ([]*ProductCatalogItem, error) {
        // 1000ä»¶ã®å•†å“ã‚’å–å¾—
        products, err := getAllProducts() // 1å›ç›®
        if err != nil {
            return nil, err
        }
        
        var catalogItems []*ProductCatalogItem
        
        for _, product := range products { // 1000å›ãƒ«ãƒ¼ãƒ—
            // å„å•†å“ã®è©³ç´°ã‚’å€‹åˆ¥å–å¾—
            details, err := getProductDetails(product.ID) // 1000å›
            if err != nil {
                continue
            }
            
            // åœ¨åº«æƒ…å ±ã‚’å–å¾—
            inventory, err := getInventory(product.ID) // 1000å›
            if err != nil {
                continue
            }
            
            // ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—
            reviews, err := getReviews(product.ID) // 1000å›
            if err != nil {
                continue
            }
            
            // å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
            for _, review := range reviews {
                reviewUser, err := getUserByID(review.UserID) // ã•ã‚‰ã«10,000å›
                if err != nil {
                    continue
                }
                review.User = reviewUser
            }
            
            // ä¾¡æ ¼å±¥æ­´ã‚’å–å¾—
            priceHistory, err := getPriceHistory(product.ID) // 1000å›
            if err != nil {
                continue
            }
            
            // é–¢é€£å•†å“ã‚’å–å¾—
            relatedProducts, err := getRelatedProducts(product.ID) // 1000å›
            if err != nil {
                continue
            }
            
            // å„é–¢é€£å•†å“ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            for _, relatedProduct := range relatedProducts {
                relatedDetails, err := getProductDetails(relatedProduct.ID) // ã•ã‚‰ã«5,000å›
                if err != nil {
                    continue
                }
                relatedProduct.Details = relatedDetails
            }
            
            catalogItems = append(catalogItems, &ProductCatalogItem{
                Product:         product,
                Details:         details,
                Inventory:       inventory,
                Reviews:         reviews,
                PriceHistory:    priceHistory,
                RelatedProducts: relatedProducts,
            })
        }
        
        // ã€ç½å®³çš„çµæœã€‘
        // - åŸºæœ¬å•†å“å–å¾—: 1å›
        // - å•†å“è©³ç´°: 1,000å›
        // - åœ¨åº«æƒ…å ±: 1,000å›
        // - ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—: 1,000å›
        // - ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼: 10,000å›ï¼ˆ1å•†å“10ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ³å®šï¼‰
        // - ä¾¡æ ¼å±¥æ­´: 1,000å›
        // - é–¢é€£å•†å“: 1,000å›
        // - é–¢é€£å•†å“è©³ç´°: 5,000å›ï¼ˆ1å•†å“5é–¢é€£æƒ³å®šï¼‰
        // åˆè¨ˆ: 21,001å›ã®ã‚¯ã‚¨ãƒªï¼
        
        return catalogItems, nil
    }
    
    // ã€å®Ÿéš›ã®è¢«å®³ä¾‹ã€‘
    // - ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ï¼šã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¡¨ç¤ºã«45ç§’â†’ãƒ¦ãƒ¼ã‚¶ãƒ¼é›¢è„±ç‡98%
    // - ECã‚µã‚¤ãƒˆï¼šå•†å“ä¸€è¦§èª­ã¿è¾¼ã¿ã«2åˆ†â†’ã‚«ãƒ¼ãƒˆæ”¾æ£„ç‡95%
    // - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼šè¨˜äº‹ä¸€è¦§è¡¨ç¤ºã«30ç§’â†’ç›´å¸°ç‡90%
    // - ä¸å‹•ç”£ã‚µã‚¤ãƒˆï¼šç‰©ä»¶æ¤œç´¢ã«1åˆ†â†’ç«¶åˆã‚µã‚¤ãƒˆã«æµå‡º
    // - æ±‚äººã‚µã‚¤ãƒˆï¼šæ±‚äººä¸€è¦§è¡¨ç¤ºã«40ç§’â†’æ¡ç”¨æ´»å‹•åœæ­¢
    
    fmt.Println("âŒ N+1 problem caused millions of queries and service collapse!")
    // çµæœï¼š1,660,001å›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“45ç§’ã€ã‚µãƒ¼ãƒãƒ¼åœæ­¢
}

// âœ… æ­£è§£ï¼šã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç´šDataLoaderã‚·ã‚¹ãƒ†ãƒ 
type EnterpriseDataLoaderSystem struct {
    // ã€åŸºæœ¬DataLoaderç®¡ç†ã€‘
    userLoader    *DataLoader[int, *User]           // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼
    postLoader    *DataLoader[int, []*Post]         // æŠ•ç¨¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    commentLoader *DataLoader[int, []*Comment]      // ã‚³ãƒ¡ãƒ³ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼
    likeLoader    *DataLoader[int, []*Like]         // ã„ã„ã­ãƒ­ãƒ¼ãƒ€ãƒ¼
    
    // ã€é«˜åº¦ãªãƒ­ãƒ¼ãƒ€ãƒ¼ã€‘
    productLoader     *DataLoader[int, *Product]           // å•†å“ãƒ­ãƒ¼ãƒ€ãƒ¼
    inventoryLoader   *DataLoader[int, *Inventory]         // åœ¨åº«ãƒ­ãƒ¼ãƒ€ãƒ¼
    reviewLoader      *DataLoader[int, []*Review]          // ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼
    priceHistoryLoader *DataLoader[int, []*PriceHistory]   // ä¾¡æ ¼å±¥æ­´ãƒ­ãƒ¼ãƒ€ãƒ¼
    
    // ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥éšå±¤ã€‘
    l1Cache       *L1Cache                         // L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰
    l2Cache       *L2Cache                         // L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰
    l3Cache       *L3Cache                         // L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆMemcachedï¼‰
    
    // ã€ãƒãƒƒãƒæœ€é©åŒ–ã€‘
    batchScheduler  *BatchScheduler               // ãƒãƒƒãƒã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    queryOptimizer  *QueryOptimizer               // ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
    indexHint       *IndexHintManager             // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ’ãƒ³ãƒˆç®¡ç†
    
    // ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã€‘
    performanceMonitor *PerformanceMonitor        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    metricsCollector   *MetricsCollector          // ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    alertManager       *AlertManager              // ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
    
    // ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆã€‘
    circuitBreaker    *CircuitBreaker             // ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼
    retryManager      *RetryManager               // ãƒªãƒˆãƒ©ã‚¤ç®¡ç†
    fallbackProvider  *FallbackProvider           // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æä¾›
    
    // ã€åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œã€‘
    distributedCache  *DistributedCache           // åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    shardingManager   *ShardingManager            // ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç®¡ç†
    
    config            *DataLoaderConfig           // è¨­å®šç®¡ç†
    mu                sync.RWMutex                // ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
}

// ã€é‡è¦é–¢æ•°ã€‘åŒ…æ‹¬çš„DataLoaderã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
func NewEnterpriseDataLoaderSystem(config *DataLoaderConfig) *EnterpriseDataLoaderSystem {
    return &EnterpriseDataLoaderSystem{
        config:             config,
        userLoader:         NewDataLoader(userBatchFn, WithMaxBatchSize[int, *User](100)),
        postLoader:         NewDataLoader(postBatchFn, WithMaxBatchSize[int, []*Post](50)),
        commentLoader:      NewDataLoader(commentBatchFn, WithMaxBatchSize[int, []*Comment](200)),
        likeLoader:         NewDataLoader(likeBatchFn, WithMaxBatchSize[int, []*Like](500)),
        productLoader:      NewDataLoader(productBatchFn, WithMaxBatchSize[int, *Product](100)),
        inventoryLoader:    NewDataLoader(inventoryBatchFn, WithMaxBatchSize[int, *Inventory](100)),
        reviewLoader:       NewDataLoader(reviewBatchFn, WithMaxBatchSize[int, []*Review](100)),
        priceHistoryLoader: NewDataLoader(priceHistoryBatchFn, WithMaxBatchSize[int, []*PriceHistory](50)),
        l1Cache:            NewL1Cache(1000),
        l2Cache:            NewL2Cache("redis://localhost:6379"),
        l3Cache:            NewL3Cache("memcached://localhost:11211"),
        batchScheduler:     NewBatchScheduler(),
        queryOptimizer:     NewQueryOptimizer(),
        indexHint:          NewIndexHintManager(),
        performanceMonitor: NewPerformanceMonitor(),
        metricsCollector:   NewMetricsCollector(),
        alertManager:       NewAlertManager(),
        circuitBreaker:     NewCircuitBreaker(),
        retryManager:       NewRetryManager(),
        fallbackProvider:   NewFallbackProvider(),
        distributedCache:   NewDistributedCache(),
        shardingManager:    NewShardingManager(),
    }
}

// ã€å®Ÿç”¨ä¾‹ã€‘æœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—
func (eds *EnterpriseDataLoaderSystem) GetOptimizedUserTimeline(
    ctx context.Context, 
    userID int,
) (*Timeline, error) {
    
    // ã€STEP 1ã€‘ãƒ•ã‚©ãƒ­ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—ï¼ˆ1å›ã®ã‚¯ã‚¨ãƒªï¼‰
    followingUsers, err := eds.getFollowingUsers(ctx, userID)
    if err != nil {
        return nil, fmt.Errorf("failed to get following users: %w", err)
    }
    
    // ã€STEP 2ã€‘å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’ä¸€æ‹¬å–å¾—ï¼ˆ1å›ã®ãƒãƒƒãƒã‚¯ã‚¨ãƒªï¼‰
    userIDs := make([]int, len(followingUsers))
    for i, user := range followingUsers {
        userIDs[i] = user.ID
    }
    
    allPosts, postErrors := eds.postLoader.LoadMany(ctx, userIDs)
    if hasErrors(postErrors) {
        return nil, fmt.Errorf("failed to load posts: %v", postErrors)
    }
    
    // ã€STEP 3ã€‘æŠ•ç¨¿IDã‚’åé›†
    var postIDs []int
    for _, posts := range allPosts {
        for _, post := range posts {
            postIDs = append(postIDs, post.ID)
        }
    }
    
    // ã€STEP 4ã€‘é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦è¡Œã§ä¸€æ‹¬å–å¾—ï¼ˆ3å›ã®ä¸¦è¡Œãƒãƒƒãƒã‚¯ã‚¨ãƒªï¼‰
    commentsCh := make(chan [][]*Comment, 1)
    likesCh := make(chan [][]*Like, 1)
    detailsCh := make(chan []*PostDetails, 1)
    errCh := make(chan error, 3)
    
    // ä¸¦è¡Œå®Ÿè¡Œã§ãƒ‡ãƒ¼ã‚¿å–å¾—
    go func() {
        comments, errors := eds.commentLoader.LoadMany(ctx, postIDs)
        if hasErrors(errors) {
            errCh <- fmt.Errorf("comment loading failed: %v", errors)
            return
        }
        commentsCh <- comments
    }()
    
    go func() {
        likes, errors := eds.likeLoader.LoadMany(ctx, postIDs)
        if hasErrors(errors) {
            errCh <- fmt.Errorf("like loading failed: %v", errors)
            return
        }
        likesCh <- likes
    }()
    
    go func() {
        details, errors := eds.postDetailsLoader.LoadMany(ctx, postIDs)
        if hasErrors(errors) {
            errCh <- fmt.Errorf("details loading failed: %v", errors)
            return
        }
        detailsCh <- details
    }()
    
    // çµæœã‚’åé›†
    var comments [][]*Comment
    var likes [][]*Like
    var details []*PostDetails
    
    for i := 0; i < 3; i++ {
        select {
        case c := <-commentsCh:
            comments = c
        case l := <-likesCh:
            likes = l
        case d := <-detailsCh:
            details = d
        case err := <-errCh:
            return nil, err
        case <-ctx.Done():
            return nil, ctx.Err()
        }
    }
    
    // ã€STEP 5ã€‘ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æ§‹ç¯‰
    timeline := &Timeline{
        UserID: userID,
        Items:  make([]*TimelineItem, 0, len(postIDs)),
    }
    
    postIndex := 0
    for _, posts := range allPosts {
        for _, post := range posts {
            timeline.Items = append(timeline.Items, &TimelineItem{
                Post:     post,
                Details:  details[postIndex],
                Comments: comments[postIndex],
                Likes:    likes[postIndex],
            })
            postIndex++
        }
    }
    
    // ã€çµæœã€‘
    // - å¾“æ¥: 1,660,001å›ã®ã‚¯ã‚¨ãƒªã€45ç§’ã®å‡¦ç†æ™‚é–“
    // - DataLoaderä½¿ç”¨: 5å›ã®ã‚¯ã‚¨ãƒªã€85ãƒŸãƒªç§’ã®å‡¦ç†æ™‚é–“
    // - æ”¹å–„ç‡: 332,000å€ã®ã‚¯ã‚¨ãƒªå‰Šæ¸›ã€529å€ã®é«˜é€ŸåŒ–
    
    return timeline, nil
}
```

DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–**ã¨**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®å‘ä¸Š**ã‚’åŒæ™‚ã«å®Ÿç¾ã™ã‚‹ç”»æœŸçš„ãªãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚Facebookï¼ˆç¾Metaï¼‰ãŒé–‹ç™ºã—ãŸGraphQLã®å®Ÿè£…ã§æ¡ç”¨ã•ã‚Œã€ç¾åœ¨ã§ã¯æ§˜ã€…ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

**å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å•é¡Œï¼š**

```go
// âŒ éåŠ¹ç‡ãªN+1å•é¡Œã®ã‚ã‚‹å®Ÿè£…
func GetUsersWithPosts(userIDs []int) ([]UserWithPosts, error) {
    var users []UserWithPosts
    
    for _, userID := range userIDs {
        // 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—ï¼ˆNå›ã®ã‚¯ã‚¨ãƒªï¼‰
        user, err := db.QueryRow("SELECT * FROM users WHERE id = ?", userID).Scan(...)
        if err != nil {
            return nil, err
        }
        
        // 2. å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’å–å¾—ï¼ˆã•ã‚‰ã«Nå›ã®ã‚¯ã‚¨ãƒªï¼‰
        posts, err := db.Query("SELECT * FROM posts WHERE user_id = ?", userID).Scan(...)
        if err != nil {
            return nil, err
        }
        
        users = append(users, UserWithPosts{User: user, Posts: posts})
    }
    
    return users // åˆè¨ˆ 1 + N + N = 2N+1 å›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
}
```

**DataLoaderã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š**

```go
// âœ… DataLoaderã«ã‚ˆã‚‹æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…
func GetUsersWithPostsOptimized(userIDs []int) ([]UserWithPosts, error) {
    var users []UserWithPosts
    
    // 1. å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä¸€æ‹¬å–å¾—ï¼ˆ1å›ã®ã‚¯ã‚¨ãƒªï¼‰
    usersData, err := userLoader.LoadMany(context.Background(), userIDs)
    if err != nil {
        return nil, err
    }
    
    // 2. å…¨æŠ•ç¨¿ã‚’ä¸€æ‹¬å–å¾—ï¼ˆ1å›ã®ã‚¯ã‚¨ãƒªï¼‰
    postsData, err := postLoader.LoadMany(context.Background(), userIDs)
    if err != nil {
        return nil, err
    }
    
    // 3. ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›
    for i, user := range usersData {
        users = append(users, UserWithPosts{
            User:  user,
            Posts: postsData[i],
        })
    }
    
    return users // åˆè¨ˆ 2å›ã®ã‚¯ã‚¨ãƒªã®ã¿
}
```

### DataLoaderã®æ ¸å¿ƒåŸç†

#### 1. **ãƒãƒƒãƒãƒ³ã‚°ï¼ˆBatchingï¼‰**

è¤‡æ•°ã®å€‹åˆ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å˜ä¸€ã®ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã«è‡ªå‹•çš„ã«çµåˆï¼š

```go
// å€‹åˆ¥ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
userLoader.Load(ctx, 1)    // SELECT * FROM users WHERE id = 1
userLoader.Load(ctx, 2)    // SELECT * FROM users WHERE id = 2
userLoader.Load(ctx, 3)    // SELECT * FROM users WHERE id = 3

// DataLoaderãŒè‡ªå‹•çš„ã«ä»¥ä¸‹ã«æœ€é©åŒ–ï¼š
// SELECT * FROM users WHERE id IN (1, 2, 3)
```

#### 2. **ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**

åŒä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…ã§ã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã‚’å®Œå…¨ã«é™¤å»ï¼š

```go
// æœ€åˆã®ã‚¢ã‚¯ã‚»ã‚¹
user1 := userLoader.Load(ctx, 1) // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰

// åŒã˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…ã§ã®å†ã‚¢ã‚¯ã‚»ã‚¹
user1Again := userLoader.Load(ctx, 1) // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å³åº§ã«è¿”å´ï¼ˆDBæœªã‚¢ã‚¯ã‚»ã‚¹ï¼‰
```

#### 3. **é…å»¶å®Ÿè¡Œï¼ˆDeferred Executionï¼‰**

ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å³åº§ã«å®Ÿè¡Œã›ãšã€æœ€é©ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒãƒƒãƒå‡¦ç†ï¼š

```go
// ã“ã‚Œã‚‰ã®å‘¼ã³å‡ºã—ã¯å³åº§ã«ã¯å®Ÿè¡Œã•ã‚Œãªã„
thunk1 := userLoader.LoadThunk(ctx, 1)
thunk2 := userLoader.LoadThunk(ctx, 2) 
thunk3 := userLoader.LoadThunk(ctx, 3)

// å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã«ãªã£ãŸæ™‚ç‚¹ã§ã€ã¾ã¨ã‚ã¦å®Ÿè¡Œ
user1, err1 := thunk1()  // ã“ã®æ™‚ç‚¹ã§ä¸€æ‹¬ã‚¯ã‚¨ãƒªãŒå®Ÿè¡Œã•ã‚Œã‚‹
user2, err2 := thunk2()  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
user3, err3 := thunk3()  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
```

### å®Œå…¨ãªDataLoaderå®Ÿè£…

#### æ ¸å¿ƒã®DataLoaderæ§‹é€ ä½“

```go
package main

import (
    "context"
    "fmt"
    "sync"
    "time"
)

// DataLoader ã¯é«˜æ€§èƒ½ãƒãƒƒãƒãƒ³ã‚°ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›
type DataLoader[K comparable, V any] struct {
    batchFn       BatchFunc[K, V]      // ãƒãƒƒãƒå‡¦ç†é–¢æ•°
    cache         map[K]*result[V]     // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    batch         []K                  // ç¾åœ¨ã®ãƒãƒƒãƒã‚­ãƒ¥ãƒ¼
    waiting       map[K][]chan *result[V] // å¾…æ©Ÿä¸­ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    maxBatchSize  int                  // æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º
    batchTimeout  time.Duration        // ãƒãƒƒãƒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    mu            sync.Mutex           // ä¸¦è¡Œåˆ¶å¾¡
    stats         *LoaderStats         // çµ±è¨ˆæƒ…å ±
}

// BatchFunc ã¯ãƒãƒƒãƒå‡¦ç†ã®é–¢æ•°å‹å®šç¾©
type BatchFunc[K comparable, V any] func(ctx context.Context, keys []K) ([]V, []error)

// result ã¯å€¤ã¨ã‚¨ãƒ©ãƒ¼ã‚’ãƒšã‚¢ã§ä¿æŒ
type result[V any] struct {
    value V
    err   error
    loadTime time.Time
}

// LoaderStats ã¯ DataLoader ã®çµ±è¨ˆæƒ…å ±
type LoaderStats struct {
    BatchCount      int64         // ãƒãƒƒãƒå®Ÿè¡Œå›æ•°
    CacheHits       int64         // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°
    CacheMisses     int64         // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ•°
    TotalLoadTime   time.Duration // ç´¯ç©ãƒ­ãƒ¼ãƒ‰æ™‚é–“
    AverageBatchSize float64      // å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º
    mu              sync.RWMutex
}

// NewDataLoader ã¯æ–°ã—ã„ DataLoader ã‚’ä½œæˆ
func NewDataLoader[K comparable, V any](
    batchFn BatchFunc[K, V],
    options ...Option[K, V],
) *DataLoader[K, V] {
    dl := &DataLoader[K, V]{
        batchFn:       batchFn,
        cache:         make(map[K]*result[V]),
        waiting:       make(map[K][]chan *result[V]),
        maxBatchSize:  100,
        batchTimeout:  16 * time.Millisecond,
        stats:         &LoaderStats{},
    }
    
    for _, opt := range options {
        opt(dl)
    }
    
    return dl
}

// Load ã¯å˜ä¸€ã‚­ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
func (dl *DataLoader[K, V]) Load(ctx context.Context, key K) (V, error) {
    return dl.LoadThunk(ctx, key)()
}

// LoadMany ã¯è¤‡æ•°ã‚­ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ­ãƒ¼ãƒ‰
func (dl *DataLoader[K, V]) LoadMany(ctx context.Context, keys []K) ([]V, []error) {
    thunks := make([]Thunk[V], len(keys))
    for i, key := range keys {
        thunks[i] = dl.LoadThunk(ctx, key)
    }
    
    values := make([]V, len(keys))
    errors := make([]error, len(keys))
    
    // ä¸¦è¡Œå®Ÿè¡Œã§åŠ¹ç‡åŒ–
    var wg sync.WaitGroup
    for i, thunk := range thunks {
        wg.Add(1)
        go func(index int, t Thunk[V]) {
            defer wg.Done()
            values[index], errors[index] = t()
        }(i, thunk)
    }
    wg.Wait()
    
    return values, errors
}

// Thunk ã¯é…å»¶å®Ÿè¡Œå¯èƒ½ãªè¨ˆç®—ã‚’è¡¨ç¾
type Thunk[V any] func() (V, error)

// LoadThunk ã¯é…å»¶å®Ÿè¡Œç”¨ã® thunk ã‚’è¿”å´
func (dl *DataLoader[K, V]) LoadThunk(ctx context.Context, key K) Thunk[V] {
    dl.mu.Lock()
    defer dl.mu.Unlock()
    
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if result, exists := dl.cache[key]; exists {
        dl.stats.recordCacheHit()
        return func() (V, error) {
            return result.value, result.err
        }
    }
    
    dl.stats.recordCacheMiss()
    
    // çµæœãƒãƒ£ãƒãƒ«ä½œæˆ
    resultCh := make(chan *result[V], 1)
    
    // å¾…æ©Ÿãƒªã‚¹ãƒˆã«è¿½åŠ 
    if dl.waiting[key] == nil {
        dl.waiting[key] = []chan *result[V]{}
        dl.batch = append(dl.batch, key)
    }
    dl.waiting[key] = append(dl.waiting[key], resultCh)
    
    // ãƒãƒƒãƒå®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼
    if len(dl.batch) >= dl.maxBatchSize {
        dl.executeImmediately(ctx)
    } else if len(dl.batch) == 1 {
        // æœ€åˆã®è¦ç´ ã§ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
        go dl.executeAfterTimeout(ctx)
    }
    
    return func() (V, error) {
        select {
        case result := <-resultCh:
            return result.value, result.err
        case <-ctx.Done():
            var zero V
            return zero, ctx.Err()
        }
    }
}

// executeImmediately ã¯ç¾åœ¨ã®ãƒãƒƒãƒã‚’å³åº§ã«å®Ÿè¡Œ
func (dl *DataLoader[K, V]) executeImmediately(ctx context.Context) {
    if len(dl.batch) == 0 {
        return
    }
    
    keys := make([]K, len(dl.batch))
    copy(keys, dl.batch)
    waiting := make(map[K][]chan *result[V])
    for k, v := range dl.waiting {
        waiting[k] = v
    }
    
    // ãƒãƒƒãƒã‚¯ãƒªã‚¢
    dl.batch = dl.batch[:0]
    dl.waiting = make(map[K][]chan *result[V])
    
    go func() {
        startTime := time.Now()
        values, errors := dl.batchFn(ctx, keys)
        loadTime := time.Since(startTime)
        
        dl.stats.recordBatch(len(keys), loadTime)
        
        for i, key := range keys {
            var result *result[V]
            if i < len(values) && i < len(errors) {
                result = &result[V]{
                    value:    values[i],
                    err:      errors[i],
                    loadTime: time.Now(),
                }
            } else {
                var zero V
                result = &result[V]{
                    value:    zero,
                    err:      fmt.Errorf("missing result for key"),
                    loadTime: time.Now(),
                }
            }
            
            // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            dl.mu.Lock()
            dl.cache[key] = result
            dl.mu.Unlock()
            
            // å¾…æ©Ÿä¸­ã®å…¨ãƒãƒ£ãƒãƒ«ã«é€ä¿¡
            for _, ch := range waiting[key] {
                select {
                case ch <- result:
                default:
                }
                close(ch)
            }
        }
    }()
}

// executeAfterTimeout ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾Œã«ãƒãƒƒãƒã‚’å®Ÿè¡Œ
func (dl *DataLoader[K, V]) executeAfterTimeout(ctx context.Context) {
    timer := time.NewTimer(dl.batchTimeout)
    defer timer.Stop()
    
    select {
    case <-timer.C:
        dl.mu.Lock()
        if len(dl.batch) > 0 {
            dl.executeImmediately(ctx)
        }
        dl.mu.Unlock()
    case <-ctx.Done():
        return
    }
}

// GetStats ã¯çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
func (dl *DataLoader[K, V]) GetStats() LoaderStats {
    return dl.stats.get()
}

// ClearCache ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
func (dl *DataLoader[K, V]) ClearCache() {
    dl.mu.Lock()
    defer dl.mu.Unlock()
    dl.cache = make(map[K]*result[V])
}

// Prime ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’äº‹å‰è¨­å®š
func (dl *DataLoader[K, V]) Prime(key K, value V) {
    dl.mu.Lock()
    defer dl.mu.Unlock()
    dl.cache[key] = &result[V]{
        value:    value,
        err:      nil,
        loadTime: time.Now(),
    }
}
```

#### çµ±è¨ˆæƒ…å ±ç®¡ç†

```go
// recordCacheHit ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’è¨˜éŒ²
func (s *LoaderStats) recordCacheHit() {
    s.mu.Lock()
    defer s.mu.Unlock()
    s.CacheHits++
}

// recordCacheMiss ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã‚’è¨˜éŒ²
func (s *LoaderStats) recordCacheMiss() {
    s.mu.Lock()
    defer s.mu.Unlock()
    s.CacheMisses++
}

// recordBatch ã¯ãƒãƒƒãƒå®Ÿè¡Œã‚’è¨˜éŒ²
func (s *LoaderStats) recordBatch(batchSize int, loadTime time.Duration) {
    s.mu.Lock()
    defer s.mu.Unlock()
    
    s.BatchCount++
    s.TotalLoadTime += loadTime
    
    // ç§»å‹•å¹³å‡ã§å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    alpha := 0.1 // å¹³æ»‘åŒ–ä¿‚æ•°
    s.AverageBatchSize = alpha*float64(batchSize) + (1-alpha)*s.AverageBatchSize
}

// get ã¯çµ±è¨ˆæƒ…å ±ã®ã‚³ãƒ”ãƒ¼ã‚’å–å¾—
func (s *LoaderStats) get() LoaderStats {
    s.mu.RLock()
    defer s.mu.RUnlock()
    return LoaderStats{
        BatchCount:       s.BatchCount,
        CacheHits:        s.CacheHits,
        CacheMisses:      s.CacheMisses,
        TotalLoadTime:    s.TotalLoadTime,
        AverageBatchSize: s.AverageBatchSize,
    }
}

// CacheHitRate ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’è¨ˆç®—
func (s *LoaderStats) CacheHitRate() float64 {
    s.mu.RLock()
    defer s.mu.RUnlock()
    
    total := s.CacheHits + s.CacheMisses
    if total == 0 {
        return 0
    }
    return float64(s.CacheHits) / float64(total)
}
```

#### è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

```go
// Option ã¯ DataLoader ã®è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³
type Option[K comparable, V any] func(*DataLoader[K, V])

// WithMaxBatchSize ã¯æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š
func WithMaxBatchSize[K comparable, V any](size int) Option[K, V] {
    return func(dl *DataLoader[K, V]) {
        if size > 0 {
            dl.maxBatchSize = size
        }
    }
}

// WithBatchTimeout ã¯ãƒãƒƒãƒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
func WithBatchTimeout[K comparable, V any](timeout time.Duration) Option[K, V] {
    return func(dl *DataLoader[K, V]) {
        if timeout > 0 {
            dl.batchTimeout = timeout
        }
    }
}

// WithCache ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’è¨­å®š
func WithCache[K comparable, V any](enabled bool) Option[K, V] {
    return func(dl *DataLoader[K, V]) {
        if !enabled {
            dl.cache = nil
        }
    }
}

// WithStats ã¯çµ±è¨ˆæƒ…å ±åé›†ã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’è¨­å®š
func WithStats[K comparable, V any](enabled bool) Option[K, V] {
    return func(dl *DataLoader[K, V]) {
        if !enabled {
            dl.stats = nil
        }
    }
}
```

### å®Ÿç”¨çš„ãªDataLoaderå®Ÿè£…ä¾‹

#### ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æŠ•ç¨¿ã‚·ã‚¹ãƒ†ãƒ ã§ã®æ´»ç”¨

```go
// User ã¨ Post ã®å®Ÿä½“
type User struct {
    ID    int    `json:"id"`
    Name  string `json:"name"`
    Email string `json:"email"`
}

type Post struct {
    ID     int    `json:"id"`
    Title  string `json:"title"`
    UserID int    `json:"user_id"`
    Body   string `json:"body"`
}

// UserLoader ã®å®Ÿè£…
type UserLoader struct {
    loader *DataLoader[int, *User]
    db     *sql.DB
}

func NewUserLoader(db *sql.DB) *UserLoader {
    batchFn := func(ctx context.Context, userIDs []int) ([]*User, []error) {
        // INå¥ã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªä¸€æ‹¬å–å¾—
        query := `SELECT id, name, email FROM users WHERE id IN (` + 
                strings.Repeat("?,", len(userIDs)-1) + "?)"
        
        args := make([]interface{}, len(userIDs))
        for i, id := range userIDs {
            args[i] = id
        }
        
        rows, err := db.QueryContext(ctx, query, args...)
        if err != nil {
            // å…¨ã¦åŒã˜ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
            errors := make([]error, len(userIDs))
            users := make([]*User, len(userIDs))
            for i := range errors {
                errors[i] = err
            }
            return users, errors
        }
        defer rows.Close()
        
        // çµæœã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        userMap := make(map[int]*User)
        for rows.Next() {
            user := &User{}
            if err := rows.Scan(&user.ID, &user.Name, &user.Email); err != nil {
                continue
            }
            userMap[user.ID] = user
        }
        
        // å…ƒã®é †åºã§çµæœã‚’æ§‹ç¯‰
        users := make([]*User, len(userIDs))
        errors := make([]error, len(userIDs))
        for i, id := range userIDs {
            if user, found := userMap[id]; found {
                users[i] = user
                errors[i] = nil
            } else {
                users[i] = nil
                errors[i] = fmt.Errorf("user not found: %d", id)
            }
        }
        
        return users, errors
    }
    
    return &UserLoader{
        loader: NewDataLoader(batchFn,
            WithMaxBatchSize[int, *User](50),
            WithBatchTimeout[int, *User](10*time.Millisecond),
        ),
        db: db,
    }
}

func (ul *UserLoader) Load(ctx context.Context, userID int) (*User, error) {
    return ul.loader.Load(ctx, userID)
}

func (ul *UserLoader) LoadMany(ctx context.Context, userIDs []int) ([]*User, []error) {
    return ul.loader.LoadMany(ctx, userIDs)
}

// PostLoader ã®å®Ÿè£…
type PostLoader struct {
    loader *DataLoader[int, []*Post]
    db     *sql.DB
}

func NewPostLoader(db *sql.DB) *PostLoader {
    batchFn := func(ctx context.Context, userIDs []int) ([][]*Post, []error) {
        query := `SELECT id, title, user_id, body FROM posts WHERE user_id IN (` +
                strings.Repeat("?,", len(userIDs)-1) + "?) ORDER BY user_id, id"
        
        args := make([]interface{}, len(userIDs))
        for i, id := range userIDs {
            args[i] = id
        }
        
        rows, err := db.QueryContext(ctx, query, args...)
        if err != nil {
            errors := make([]error, len(userIDs))
            posts := make([][]*Post, len(userIDs))
            for i := range errors {
                errors[i] = err
                posts[i] = []*Post{}
            }
            return posts, errors
        }
        defer rows.Close()
        
        // ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã”ã¨ã«æŠ•ç¨¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        postMap := make(map[int][]*Post)
        for rows.Next() {
            post := &Post{}
            if err := rows.Scan(&post.ID, &post.Title, &post.UserID, &post.Body); err != nil {
                continue
            }
            postMap[post.UserID] = append(postMap[post.UserID], post)
        }
        
        // çµæœã‚’æ§‹ç¯‰
        posts := make([][]*Post, len(userIDs))
        errors := make([]error, len(userIDs))
        for i, userID := range userIDs {
            if userPosts, found := postMap[userID]; found {
                posts[i] = userPosts
            } else {
                posts[i] = []*Post{} // ç©ºã®ã‚¹ãƒ©ã‚¤ã‚¹
            }
            errors[i] = nil
        }
        
        return posts, errors
    }
    
    return &PostLoader{
        loader: NewDataLoader(batchFn,
            WithMaxBatchSize[int, []*Post](30),
            WithBatchTimeout[int, []*Post](15*time.Millisecond),
        ),
        db: db,
    }
}

func (pl *PostLoader) Load(ctx context.Context, userID int) ([]*Post, error) {
    return pl.loader.Load(ctx, userID)
}
```

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ 

```go
// LoaderMetrics ã¯è¤‡æ•°ã®DataLoaderã®çµ±è¨ˆã‚’é›†è¨ˆ
type LoaderMetrics struct {
    loaders map[string]StatProvider
    mu      sync.RWMutex
}

type StatProvider interface {
    GetStats() LoaderStats
}

func NewLoaderMetrics() *LoaderMetrics {
    return &LoaderMetrics{
        loaders: make(map[string]StatProvider),
    }
}

func (lm *LoaderMetrics) RegisterLoader(name string, loader StatProvider) {
    lm.mu.Lock()
    defer lm.mu.Unlock()
    lm.loaders[name] = loader
}

func (lm *LoaderMetrics) GetAggregatedStats() map[string]LoaderStats {
    lm.mu.RLock()
    defer lm.mu.RUnlock()
    
    stats := make(map[string]LoaderStats)
    for name, loader := range lm.loaders {
        stats[name] = loader.GetStats()
    }
    return stats
}

func (lm *LoaderMetrics) PrintReport() {
    stats := lm.GetAggregatedStats()
    
    fmt.Println("=== DataLoader Performance Report ===")
    for name, stat := range stats {
        fmt.Printf("\n%s:\n", name)
        fmt.Printf("  Batches: %d\n", stat.BatchCount)
        fmt.Printf("  Cache Hit Rate: %.2f%%\n", stat.CacheHitRate()*100)
        fmt.Printf("  Avg Batch Size: %.1f\n", stat.AverageBatchSize)
        if stat.BatchCount > 0 {
            avgTime := stat.TotalLoadTime / time.Duration(stat.BatchCount)
            fmt.Printf("  Avg Load Time: %v\n", avgTime)
        }
    }
}
```

#### GraphQLçµ±åˆä¾‹

```go
// GraphQL ãƒªã‚¾ãƒ«ãƒã§ã®DataLoaderæ´»ç”¨
type Resolvers struct {
    userLoader *UserLoader
    postLoader *PostLoader
}

func (r *Resolvers) User(ctx context.Context, id int) (*User, error) {
    return r.userLoader.Load(ctx, id)
}

func (r *Resolvers) UserPosts(ctx context.Context, user *User) ([]*Post, error) {
    return r.postLoader.Load(ctx, user.ID)
}

// è¤‡é›‘ãªã‚¯ã‚¨ãƒªã§ã®åŠ¹æœ
func (r *Resolvers) UsersWithPosts(ctx context.Context, userIDs []int) ([]*UserWithPosts, error) {
    // ä¸¦è¡Œã—ã¦ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    usersCh := make(chan []*User, 1)
    postsCh := make(chan [][]*Post, 1)
    errCh := make(chan error, 2)
    
    go func() {
        users, errs := r.userLoader.LoadMany(ctx, userIDs)
        for _, err := range errs {
            if err != nil {
                errCh <- err
                return
            }
        }
        usersCh <- users
    }()
    
    go func() {
        posts, errs := r.postLoader.LoadMany(ctx, userIDs)
        for _, err := range errs {
            if err != nil {
                errCh <- err
                return
            }
        }
        postsCh <- posts
    }()
    
    var users []*User
    var posts [][]*Post
    
    for i := 0; i < 2; i++ {
        select {
        case u := <-usersCh:
            users = u
        case p := <-postsCh:
            posts = p
        case err := <-errCh:
            return nil, err
        case <-ctx.Done():
            return nil, ctx.Err()
        }
    }
    
    // çµæœã‚’çµåˆ
    result := make([]*UserWithPosts, len(userIDs))
    for i := range userIDs {
        result[i] = &UserWithPosts{
            User:  users[i],
            Posts: posts[i],
        }
    }
    
    return result, nil
}

type UserWithPosts struct {
    User  *User   `json:"user"`
    Posts []*Post `json:"posts"`
}
```

## ğŸ“ èª²é¡Œï¼ˆå®Ÿè£…è¦ä»¶ï¼‰

`main_test.go`ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹ã‚ˆã†ã«ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

### 1. **æ±ç”¨DataLoaderã‚·ã‚¹ãƒ†ãƒ **
- ã‚¸ã‚§ãƒãƒªã‚¯ã‚¹ã‚’æ´»ç”¨ã—ãŸå‹å®‰å…¨ãªå®Ÿè£…
- ãƒãƒƒãƒãƒ³ã‚°ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ»é…å»¶å®Ÿè¡Œã®å®Œå…¨ã‚µãƒãƒ¼ãƒˆ
- çµ±è¨ˆæƒ…å ±åé›†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### 2. **UserLoaderï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å°‚ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼‰**
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŠ¹ç‡çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€æ‹¬å–å¾—
- æ¬ æãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ´»ç”¨

### 3. **PostLoaderï¼ˆæŠ•ç¨¿å°‚ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼‰**
- ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ã‚­ãƒ¼ã¨ã—ãŸæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬å–å¾—
- 1å¯¾å¤šé–¢ä¿‚ã®åŠ¹ç‡çš„ãªå‡¦ç†
- ç©ºã®çµæœã‚»ãƒƒãƒˆã®é©åˆ‡ãªå‡¦ç†

### 4. **çµ±åˆãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª**
- N+1å•é¡Œã®å›é¿æ¤œè¨¼
- ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®å®‰å…¨æ€§ç¢ºèª
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœã®æ¸¬å®š

**å®Ÿè£…ã™ã¹ãé–¢æ•°ï¼š**

```go
// æ±ç”¨DataLoader
func NewDataLoader[K comparable, V any](batchFn BatchFunc[K, V], options ...Option[K, V]) *DataLoader[K, V]
func (dl *DataLoader[K, V]) Load(ctx context.Context, key K) (V, error)
func (dl *DataLoader[K, V]) LoadMany(ctx context.Context, keys []K) ([]V, []error)
func (dl *DataLoader[K, V]) GetStats() LoaderStats

// å°‚ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼
func NewUserLoader(db *sql.DB) *UserLoader
func NewPostLoader(db *sql.DB) *PostLoader
```

## âœ… æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹•

å®Ÿè£…ãŒæ­£ã—ãå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¾‹
```bash
$ go test -v
=== RUN   TestDataLoader_Load
=== RUN   TestDataLoader_Load/Single_user_load
=== RUN   TestDataLoader_Load/Cache_effectiveness
--- PASS: TestDataLoader_Load (0.01s)

=== RUN   TestDataLoader_LoadMany
=== RUN   TestDataLoader_LoadMany/Batch_loading
=== RUN   TestDataLoader_LoadMany/Mixed_cache_miss_hit
--- PASS: TestDataLoader_LoadMany (0.02s)

=== RUN   TestDataLoader_Batch
=== RUN   TestDataLoader_Batch/Automatic_batching
=== RUN   TestDataLoader_Batch/Timeout_batching
--- PASS: TestDataLoader_Batch (0.02s)

=== RUN   TestUserLoader_Integration
=== RUN   TestUserLoader_Integration/N_plus_one_prevention
--- PASS: TestUserLoader_Integration (0.01s)

=== RUN   TestPostLoader_Integration
=== RUN   TestPostLoader_Integration/User_posts_batching
--- PASS: TestPostLoader_Integration (0.01s)

PASS
ok      day36-dataloader    0.075s
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¾‹
```bash
$ go test -bench=.
BenchmarkDataLoader_Load-8           5000000    250 ns/op      48 B/op    2 allocs/op
BenchmarkDataLoader_LoadMany-8       1000000   1500 ns/op     256 B/op    8 allocs/op
BenchmarkUserLoader_NPlus1-8           10000 100000 ns/op    2048 B/op   50 allocs/op
BenchmarkUserLoader_Optimized-8      500000   3000 ns/op     512 B/op   10 allocs/op
```

### å®Ÿè¡Œæ™‚ãƒ­ã‚°ä¾‹
```
=== DataLoader Performance Report ===

UserLoader:
  Batches: 15
  Cache Hit Rate: 73.50%
  Avg Batch Size: 8.2
  Avg Load Time: 2.5ms

PostLoader:
  Batches: 12
  Cache Hit Rate: 45.20%
  Avg Batch Size: 12.1
  Avg Load Time: 4.1ms

Performance Improvement:
  Traditional N+1: 250 queries in 1.2s
  DataLoader: 27 batches in 85ms
  Speed Improvement: 14.1x
```

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã®ãƒ’ãƒ³ãƒˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

### åŸºæœ¬çš„ãªãƒãƒƒãƒé–¢æ•°ã®å®Ÿè£…
```go
func userBatchFn(ctx context.Context, userIDs []int) ([]*User, []error) {
    // 1. INå¥ã§ã¾ã¨ã‚ã¦å–å¾—
    query := "SELECT id, name, email FROM users WHERE id IN (...)"
    
    // 2. çµæœã‚’ãƒãƒƒãƒ—ã«æ ¼ç´
    userMap := make(map[int]*User)
    
    // 3. å…ƒã®é †åºã§çµæœã‚’å†æ§‹ç¯‰
    users := make([]*User, len(userIDs))
    errors := make([]error, len(userIDs))
    
    return users, errors
}
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨æ–¹æ³•
```go
// äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
userLoader.Prime(1, &User{ID: 1, Name: "Alice"})

// ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®ãƒ’ãƒƒãƒˆ
user, err := userLoader.Load(ctx, 1) // DBæœªã‚¢ã‚¯ã‚»ã‚¹
```

### çµ±è¨ˆæƒ…å ±ã®æ´»ç”¨
```go
stats := userLoader.GetStats()
fmt.Printf("Cache Hit Rate: %.2f%%\n", stats.CacheHitRate()*100)
```

**é‡è¦ãªå®Ÿè£…ãƒã‚¤ãƒ³ãƒˆï¼š**
- **ä¸¦è¡Œå®‰å…¨æ€§**: `sync.Mutex`ã‚’é©åˆ‡ã«ä½¿ç”¨
- **ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†**: goroutineãƒªãƒ¼ã‚¯ã®é˜²æ­¢
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å€‹åˆ¥ã‚¨ãƒ©ãƒ¼ã¨ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã®åŒºåˆ¥
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã®å›é¿

## å®Ÿè¡Œæ–¹æ³•

```bash
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æº–å‚™
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=test -e POSTGRES_DB=testdb postgres:15

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
go test -v
go test -race          # ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ¤œå‡º
go test -bench=.       # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®š
go test -cover         # ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèª
```
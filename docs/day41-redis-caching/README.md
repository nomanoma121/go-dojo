# Day 41: Redisé«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

## ğŸ¯ æœ¬æ—¥ã®ç›®æ¨™

ã“ã®ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’é€šã—ã¦ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

- **Redisã‚’æ´»ç”¨ã—ãŸé«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹**
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸¡ç«‹ã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹**
- **Redis Clusteræ§‹æˆã§ã®åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹**
- **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®Redisé‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¿’å¾—ã™ã‚‹**

## ğŸ“– è§£èª¬

### ãªãœRedisã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¿…è¦ãªã®ã‹ï¼Ÿ

ç¾ä»£ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ã‚¯ã‚¨ãƒªãŒæ€§èƒ½ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚é©åˆ‡ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ãªã—ã§ã¯ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š

#### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®æ€§èƒ½å•é¡Œ

```go
// âŒ ã€è‡´å‘½çš„å•é¡Œã€‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®é‡è¤‡ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹æ€§èƒ½åŠ£åŒ–
func GetUserProfile(db *sql.DB, userID int) (*UserProfile, error) {
    // ã€ç½å®³ã‚·ãƒŠãƒªã‚ªã€‘æ¯å›ã®é‡ã„JOINã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    query := `
        SELECT u.id, u.name, u.email, u.avatar_url,
               p.bio, p.website, p.location,
               COUNT(f.follower_id) as follower_count,
               COUNT(po.id) as post_count,
               AVG(r.rating) as avg_rating
        FROM users u
        LEFT JOIN profiles p ON u.id = p.user_id
        LEFT JOIN followers f ON u.id = f.user_id
        LEFT JOIN posts po ON u.id = po.author_id
        LEFT JOIN reviews r ON u.id = r.reviewer_id
        WHERE u.id = $1
        GROUP BY u.id, p.bio, p.website, p.location
    `
    
    // ã€æ€§èƒ½å•é¡Œã®è©³ç´°åˆ†æã€‘ï¼š
    // 
    // 1. ã‚¯ã‚¨ãƒªã‚³ã‚¹ãƒˆ: 300ms/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    //    - 5ãƒ†ãƒ¼ãƒ–ãƒ«ã®JOINå‡¦ç†
    //    - GROUP BY ã«ã‚ˆã‚‹é›†è¨ˆè¨ˆç®—
    //    - COUNT, AVG ã®é‡ã„çµ±è¨ˆå‡¦ç†
    //
    // 2. åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®ç ´ç¶»:
    //    - 100åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ Ã— 300ms = 30ç§’ã®ç´¯ç©å¾…æ©Ÿæ™‚é–“
    //    - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®æ¯æ¸‡
    //    - CPUã¨I/Oãƒªã‚½ãƒ¼ã‚¹ã®æ¯æ¸‡
    //    - ä»–ã®ã‚¯ã‚¨ãƒªã¸ã®é€£é–çš„å½±éŸ¿
    //
    // 3. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®é™ç•Œ:
    //    - åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ1åˆ†é–“ã«ä½•åº¦ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ â†’ ç„¡é§„ãªé‡è¤‡å‡¦ç†
    //    - äººæ°—ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹é›†ä¸­ â†’ ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆå•é¡Œ
    //    - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ 
    
    start := time.Now()
    row := db.QueryRow(query, userID)
    
    var profile UserProfile
    err := row.Scan(
        &profile.ID, &profile.Name, &profile.Email, &profile.AvatarURL,
        &profile.Bio, &profile.Website, &profile.Location,
        &profile.FollowerCount, &profile.PostCount, &profile.AvgRating,
    )
    if err != nil {
        return nil, err
    }
    
    // ã€å®Ÿæ¸¬å€¤ä¾‹ã€‘æœ¬ç•ªç’°å¢ƒã§ã®å®Ÿéš›ã®æ€§èƒ½åŠ£åŒ–
    duration := time.Since(start)
    log.Printf("ğŸŒ SLOW DATABASE QUERY: User %d took %v", userID, duration)
    
    // ã€å®Ÿéš›ã®å•é¡Œäº‹ä¾‹ã€‘ï¼š
    // - EC2 r5.large: ã‚¯ã‚¨ãƒªæ™‚é–“ 450ms â†’ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¿œç­”æ€§æ‚ªåŒ–
    // - RDS PostgreSQL: æ¥ç¶šæ•°ä¸Šé™åˆ°é” â†’ "too many connections" ã‚¨ãƒ©ãƒ¼
    // - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒãƒ¼: CPU 90%ä½¿ç”¨ç‡ â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹åœæ­¢
    // - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“: ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ 5ç§’ä»¥ä¸Š â†’ é›¢è„±ç‡ 40%å¢—åŠ 
    
    return &profile, nil
}

func GetPopularPosts(db *sql.DB, limit int) ([]Post, error) {
    // æ¯å›é‡ã„é›†è¨ˆã‚¯ã‚¨ãƒª
    query := `
        SELECT p.id, p.title, p.content, p.created_at,
               u.name as author_name,
               COUNT(l.id) as like_count,
               COUNT(c.id) as comment_count
        FROM posts p
        JOIN users u ON p.author_id = u.id
        LEFT JOIN likes l ON p.id = l.post_id
        LEFT JOIN comments c ON p.id = c.post_id
        WHERE p.created_at > NOW() - INTERVAL '24 hours'
        GROUP BY p.id, u.name
        ORDER BY like_count DESC, comment_count DESC
        LIMIT $1
    `
    
    // äººæ°—æŠ•ç¨¿ã®è¨ˆç®—ã«æ¯å›5ç§’ã‹ã‹ã‚‹
    rows, err := db.Query(query, limit)
    // ...å‡¦ç†
}
```

**å•é¡Œç‚¹ã®åˆ†æï¼š**
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹300ms-5ç§’ã®é…å»¶
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è² è·**: åŒã˜ã‚¯ã‚¨ãƒªãŒä½•åº¦ã‚‚å®Ÿè¡Œã•ã‚ŒCPU/IOã‚’åœ§è¿«
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ã‚¢ã‚¯ã‚»ã‚¹æ•°å¢—åŠ ã§æŒ‡æ•°çš„ã«æ€§èƒ½åŠ£åŒ–
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“**: é…ã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ã‚ˆã‚‹é›¢è„±ç‡å¢—åŠ 

### Redisã«ã‚ˆã‚‹åŠ‡çš„ãªæ€§èƒ½æ”¹å–„

åŒã˜æ©Ÿèƒ½ã‚’Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æœ€é©åŒ–ã™ã‚‹ã¨ï¼š

```go
import (
    "github.com/redis/go-redis/v9"
    "encoding/json"
    "time"
    "context"
)

type CacheManager struct {
    rdb           *redis.Client
    db            *sql.DB
    defaultTTL    time.Duration
    clusterClient *redis.ClusterClient
    metrics       *CacheMetrics
}

type CacheMetrics struct {
    Hits          int64
    Misses        int64
    Evictions     int64
    TotalRequests int64
    AvgLatency    time.Duration
    mu            sync.RWMutex
}

func NewCacheManager(redisAddr, dbDSN string) (*CacheManager, error) {
    // ã€æœ€é©åŒ–ã•ã‚ŒãŸRedisæ¥ç¶šè¨­å®šã€‘ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œã®è©³ç´°è¨­å®š
    rdb := redis.NewClient(&redis.Options{
        Addr:     redisAddr,
        Password: "",
        DB:       0,
        
        // ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®šã€‘é«˜è² è·å¯¾å¿œ
        PoolSize:        100,           // ã€é‡è¦ã€‘æœ€å¤§æ¥ç¶šæ•° - CPUã‚³ã‚¢æ•°Ã—10-20ãŒç›®å®‰
        PoolTimeout:     30 * time.Second, // ãƒ—ãƒ¼ãƒ«å¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        IdleTimeout:     5 * time.Minute,  // ã‚¢ã‚¤ãƒ‰ãƒ«æ¥ç¶šã®ä¿æŒæ™‚é–“
        IdleCheckFrequency: 1 * time.Minute, // ã‚¢ã‚¤ãƒ‰ãƒ«æ¥ç¶šã®ãƒã‚§ãƒƒã‚¯é–“éš”
        
        // ã€æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã€‘ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ€§èƒ½ã¨ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒãƒ©ãƒ³ã‚¹
        DialTimeout:  10 * time.Second, // ã€æ¥ç¶šç¢ºç«‹ã€‘æ–°è¦æ¥ç¶šã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        ReadTimeout:  5 * time.Second,  // ã€èª­ã¿å–ã‚Šã€‘å¤§ããªãƒ‡ãƒ¼ã‚¿è»¢é€ã‚‚è€ƒæ…®
        WriteTimeout: 5 * time.Second,  // ã€æ›¸ãè¾¼ã¿ã€‘ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã‚’è€ƒæ…®
        
        // ã€å†è©¦è¡Œè¨­å®šã€‘ä¸€æ™‚çš„éšœå®³ã¸ã®å¯¾å¿œ
        MaxRetries:      3,                      // æœ€å¤§å†è©¦è¡Œå›æ•°
        MinRetryBackoff: 100 * time.Millisecond, // æœ€å°ãƒãƒƒã‚¯ã‚ªãƒ•æ™‚é–“
        MaxRetryBackoff: 2 * time.Second,        // æœ€å¤§ãƒãƒƒã‚¯ã‚ªãƒ•æ™‚é–“
        
        // ã€è¨­å®šã®æ ¹æ‹ ã€‘ï¼š
        // 1. PoolSize=100: é€šå¸¸ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ååˆ†ãªä¸¦è¡Œæ¥ç¶šæ•°
        // 2. ReadTimeout=5s: JSONãƒ‡ãƒ¼ã‚¿100KBç¨‹åº¦ãªã‚‰ååˆ†
        // 3. MaxRetries=3: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¬æ–­ã¸ã®å¯¾å¿œ
        // 4. MinRetryBackoff=100ms: é«˜é€Ÿãƒªãƒˆãƒ©ã‚¤ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ç¶­æŒ
    })
    
    // æ¥ç¶šãƒ†ã‚¹ãƒˆ
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    if err := rdb.Ping(ctx).Err(); err != nil {
        return nil, fmt.Errorf("failed to connect to Redis: %w", err)
    }
    
    db, err := sql.Open("postgres", dbDSN)
    if err != nil {
        return nil, fmt.Errorf("failed to connect to database: %w", err)
    }
    
    return &CacheManager{
        rdb:        rdb,
        db:         db,
        defaultTTL: 15 * time.Minute,
        metrics:    &CacheMetrics{},
    }, nil
}

func (cm *CacheManager) GetUserProfile(ctx context.Context, userID int) (*UserProfile, error) {
    start := time.Now()
    defer func() {
        cm.recordLatency(time.Since(start))
    }()
    
    // ã€ã‚­ãƒ¼è¨­è¨ˆã€‘éšå±¤çš„ãªå‘½åè¦å‰‡ã§ç®¡ç†æ€§å‘ä¸Š
    cacheKey := fmt.Sprintf("user_profile:%d", userID)
    
    // ã€STEP 1ã€‘L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰ã‹ã‚‰ã®é«˜é€Ÿå–å¾—
    cached, err := cm.rdb.Get(ctx, cacheKey).Result()
    if err == nil {
        // ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã€‘2-5ms ã§å¿œç­”å®Œäº†
        cm.recordHit()
        
        var profile UserProfile
        if err := json.Unmarshal([]byte(cached), &profile); err != nil {
            // ã€ãƒ‡ãƒ¼ã‚¿ç ´æå¯¾å¿œã€‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ãªå ´åˆã®å‡¦ç†
            log.Printf("âš ï¸  Cache data corruption for user %d: %v", userID, err)
            cm.recordCacheCorruption()
            
            // ç ´æãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ DB ã‹ã‚‰å†å–å¾—
            cm.rdb.Del(ctx, cacheKey)
            return cm.getUserProfileFromDB(ctx, userID)
        }
        
        // ã€æˆåŠŸãƒ­ã‚°ã€‘æ€§èƒ½ç›£è¦–ç”¨
        log.Printf("âš¡ CACHE HIT: User %d retrieved in %v", userID, time.Since(start))
        return &profile, nil
    }
    
    // ã€STEP 2ã€‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã®å‡¦ç†åˆ†å²
    if err != redis.Nil {
        // ã€Rediséšœå®³æ™‚ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ã‚µãƒ¼ãƒ“ã‚¹ç¶™ç¶š
        log.Printf("ğŸš¨ Redis error for user %d: %v", userID, err)
        cm.recordRedisError()
        
        // Redis éšœå®³æ™‚ã‚‚ã‚µãƒ¼ãƒ“ã‚¹ç¶™ç¶šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ï¼‰
        return cm.getUserProfileFromDB(ctx, userID)
    }
    
    // ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
    cm.recordMiss()
    log.Printf("ğŸ’¾ CACHE MISS: User %d - fetching from database", userID)
    
    profile, err := cm.getUserProfileFromDB(ctx, userID)
    if err != nil {
        return nil, err
    }
    
    // ã€STEP 3ã€‘éåŒæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°ã§å¿œç­”æ€§èƒ½ã‚’ç¶­æŒ
    go func() {
        // ã€éåŒæœŸå‡¦ç†ã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
        cacheCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
        defer cancel()
        
        if err := cm.cacheUserProfile(cacheCtx, cacheKey, profile); err != nil {
            log.Printf("âŒ Failed to cache user profile %d: %v", userID, err)
            cm.recordCacheWriteError()
        } else {
            log.Printf("ğŸ’¾ Successfully cached user profile %d", userID)
        }
    }()
    
    // ã€æ€§èƒ½ãƒ­ã‚°ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã‚’è¨˜éŒ²
    log.Printf("ğŸ’¾ DATABASE FETCH: User %d completed in %v", userID, time.Since(start))
    return profile, nil
}

// ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ ã€‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã®è©³ç´°ç›£è¦–
func (cm *CacheManager) recordHit() {
    atomic.AddInt64(&cm.metrics.Hits, 1)
    atomic.AddInt64(&cm.metrics.TotalRequests, 1)
}

func (cm *CacheManager) recordMiss() {
    atomic.AddInt64(&cm.metrics.Misses, 1)
    atomic.AddInt64(&cm.metrics.TotalRequests, 1)
}

func (cm *CacheManager) recordCacheCorruption() {
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ç ´æã®ç›£è¦–ï¼ˆJSON parse ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰
    log.Printf("âš ï¸  Cache corruption detected - monitoring required")
}

func (cm *CacheManager) recordRedisError() {
    // Redis æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®ç›£è¦–
    log.Printf("ğŸš¨ Redis connection error - check Redis server health")
}

func (cm *CacheManager) recordCacheWriteError() {
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã®ç›£è¦–
    log.Printf("âŒ Cache write operation failed - check Redis capacity")
}

func (cm *CacheManager) recordLatency(duration time.Duration) {
    // ã€æ€§èƒ½ç›£è¦–ã€‘ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®è¨˜éŒ²
    cm.metrics.mu.Lock()
    defer cm.metrics.mu.Unlock()
    
    // ç§»å‹•å¹³å‡ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆç®—
    if cm.metrics.AvgLatency == 0 {
        cm.metrics.AvgLatency = duration
    } else {
        // æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆÎ±=0.1ï¼‰
        cm.metrics.AvgLatency = time.Duration(
            float64(cm.metrics.AvgLatency)*0.9 + float64(duration)*0.1,
        )
    }
}

func (cm *CacheManager) getUserProfileFromDB(ctx context.Context, userID int) (*UserProfile, error) {
    query := `
        SELECT u.id, u.name, u.email, u.avatar_url,
               COALESCE(p.bio, '') as bio, 
               COALESCE(p.website, '') as website, 
               COALESCE(p.location, '') as location,
               COALESCE(stats.follower_count, 0) as follower_count,
               COALESCE(stats.post_count, 0) as post_count,
               COALESCE(stats.avg_rating, 0) as avg_rating
        FROM users u
        LEFT JOIN profiles p ON u.id = p.user_id
        LEFT JOIN (
            SELECT user_id,
                   COUNT(DISTINCT f.follower_id) as follower_count,
                   COUNT(DISTINCT po.id) as post_count,
                   AVG(r.rating) as avg_rating
            FROM users u2
            LEFT JOIN followers f ON u2.id = f.user_id
            LEFT JOIN posts po ON u2.id = po.author_id
            LEFT JOIN reviews r ON u2.id = r.reviewer_id
            WHERE u2.id = $1
            GROUP BY user_id
        ) stats ON u.id = stats.user_id
        WHERE u.id = $1
    `
    
    var profile UserProfile
    err := cm.db.QueryRowContext(ctx, query, userID).Scan(
        &profile.ID, &profile.Name, &profile.Email, &profile.AvatarURL,
        &profile.Bio, &profile.Website, &profile.Location,
        &profile.FollowerCount, &profile.PostCount, &profile.AvgRating,
    )
    
    if err != nil {
        return nil, fmt.Errorf("failed to get user profile from DB: %w", err)
    }
    
    return &profile, nil
}

func (cm *CacheManager) cacheUserProfile(ctx context.Context, key string, profile *UserProfile) error {
    data, err := json.Marshal(profile)
    if err != nil {
        return fmt.Errorf("failed to marshal profile: %w", err)
    }
    
    return cm.rdb.Set(ctx, key, data, cm.defaultTTL).Err()
}
```

**æ”¹å–„åŠ¹æœï¼š**
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: 300ms â†’ 2msï¼ˆ150å€é«˜é€ŸåŒ–ï¼‰
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è² è·**: 95%å‰Šæ¸›ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡90%æƒ³å®šï¼‰
- **åŒæ™‚å‡¦ç†èƒ½åŠ›**: 10ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ç§’ â†’ 5000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ç§’
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“**: ç¬æ™‚ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ã‚ˆã‚‹æº€è¶³åº¦å‘ä¸Š

### é«˜åº¦ãªRedisã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æˆ¦ç•¥

```go
type MultiLayerCache struct {
    l1Cache    *sync.Map          // ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†…ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
    l2Cache    *redis.Client      // Redisåˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    db         *sql.DB            // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    preloader  *CachePreloader
}

type CachePreloader struct {
    cache     *MultiLayerCache
    scheduler *time.Ticker
    patterns  []PreloadPattern
}

type PreloadPattern struct {
    KeyPattern    string
    LoadFunction  func(ctx context.Context) (map[string]interface{}, error)
    Schedule      time.Duration
    Priority      int
}

func (mlc *MultiLayerCache) Get(ctx context.Context, key string) (interface{}, error) {
    // L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã‹ã‚‰è©¦è¡Œ
    if value, ok := mlc.l1Cache.Load(key); ok {
        return value, nil
    }
    
    // L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰ã‹ã‚‰è©¦è¡Œ
    redisValue, err := mlc.l2Cache.Get(ctx, key).Result()
    if err == nil {
        var data interface{}
        if err := json.Unmarshal([]byte(redisValue), &data); err == nil {
            // L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜
            mlc.l1Cache.Store(key, data)
            return data, nil
        }
    }
    
    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    return mlc.loadFromDatabase(ctx, key)
}

func (mlc *MultiLayerCache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    // L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    mlc.l1Cache.Store(key, value)
    
    // L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰ã«ä¿å­˜
    data, err := json.Marshal(value)
    if err != nil {
        return err
    }
    
    return mlc.l2Cache.Set(ctx, key, data, ttl).Err()
}

func (cp *CachePreloader) StartPreloading() {
    go func() {
        for range cp.scheduler.C {
            for _, pattern := range cp.patterns {
                go cp.preloadPattern(pattern)
            }
        }
    }()
}

func (cp *CachePreloader) preloadPattern(pattern PreloadPattern) {
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    data, err := pattern.LoadFunction(ctx)
    if err != nil {
        log.Printf("Failed to preload pattern %s: %v", pattern.KeyPattern, err)
        return
    }
    
    for key, value := range data {
        if err := cp.cache.Set(ctx, key, value, pattern.Schedule*2); err != nil {
            log.Printf("Failed to cache preloaded data %s: %v", key, err)
        }
    }
    
    log.Printf("Preloaded %d items for pattern %s", len(data), pattern.KeyPattern)
}

// äººæ°—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¾‹
func (cm *CacheManager) PreloadPopularContent(ctx context.Context) (map[string]interface{}, error) {
    popularPosts, err := cm.getPopularPostsFromDB(ctx, 100)
    if err != nil {
        return nil, err
    }
    
    trendingUsers, err := cm.getTrendingUsersFromDB(ctx, 50)
    if err != nil {
        return nil, err
    }
    
    result := make(map[string]interface{})
    
    for _, post := range popularPosts {
        key := fmt.Sprintf("post:%d", post.ID)
        result[key] = post
    }
    
    for _, user := range trendingUsers {
        key := fmt.Sprintf("user_profile:%d", user.ID)
        result[key] = user
    }
    
    return result, nil
}
```

#### 2. Redis Pipelineã¨ãƒãƒƒãƒå‡¦ç†

```go
type BatchCacheManager struct {
    rdb         *redis.Client
    batchSize   int
    flushTimer  *time.Timer
    pending     map[string]CacheOperation
    mu          sync.Mutex
}

type CacheOperation struct {
    Type      string      // "SET", "GET", "DEL"
    Key       string
    Value     interface{}
    TTL       time.Duration
    Callback  func(interface{}, error)
    CreatedAt time.Time
}

func (bcm *BatchCacheManager) BatchSet(key string, value interface{}, ttl time.Duration) {
    bcm.mu.Lock()
    defer bcm.mu.Unlock()
    
    bcm.pending[key] = CacheOperation{
        Type:      "SET",
        Key:       key,
        Value:     value,
        TTL:       ttl,
        CreatedAt: time.Now(),
    }
    
    if len(bcm.pending) >= bcm.batchSize {
        go bcm.flushPending()
    }
}

func (bcm *BatchCacheManager) flushPending() {
    bcm.mu.Lock()
    operations := make(map[string]CacheOperation)
    for k, v := range bcm.pending {
        operations[k] = v
        delete(bcm.pending, k)
    }
    bcm.mu.Unlock()
    
    if len(operations) == 0 {
        return
    }
    
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // Redisãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä¸€æ‹¬å®Ÿè¡Œ
    pipe := bcm.rdb.Pipeline()
    
    for _, op := range operations {
        switch op.Type {
        case "SET":
            data, err := json.Marshal(op.Value)
            if err != nil {
                log.Printf("Failed to marshal value for key %s: %v", op.Key, err)
                continue
            }
            pipe.Set(ctx, op.Key, data, op.TTL)
            
        case "DEL":
            pipe.Del(ctx, op.Key)
        }
    }
    
    // ä¸€æ‹¬å®Ÿè¡Œ
    cmds, err := pipe.Exec(ctx)
    if err != nil {
        log.Printf("Failed to execute batch operations: %v", err)
        return
    }
    
    log.Printf("Executed %d cache operations in batch", len(cmds))
}

func (bcm *BatchCacheManager) StartBatchProcessor() {
    bcm.flushTimer = time.NewTimer(100 * time.Millisecond)
    go func() {
        for range bcm.flushTimer.C {
            bcm.flushPending()
            bcm.flushTimer.Reset(100 * time.Millisecond)
        }
    }()
}
```

#### 3. Redis Clusteråˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥

```go
type ClusterCacheManager struct {
    cluster     *redis.ClusterClient
    hashRing    *ConsistentHash
    nodeManager *NodeManager
    metrics     *ClusterMetrics
}

type ConsistentHash struct {
    nodes    map[uint32]string
    keys     []uint32
    replicas int
    mu       sync.RWMutex
}

type NodeManager struct {
    nodes       []string
    healthCheck map[string]bool
    mu          sync.RWMutex
}

type ClusterMetrics struct {
    NodeLatency    map[string]time.Duration
    NodeLoad       map[string]int64
    RedirectCount  int64
    ClusterErrors  int64
    mu             sync.RWMutex
}

func NewClusterCacheManager(nodes []string) (*ClusterCacheManager, error) {
    // Redis Clusterã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
    cluster := redis.NewClusterClient(&redis.ClusterOptions{
        Addrs:              nodes,
        MaxRedirects:       3,
        ReadOnly:           true,
        RouteByLatency:     true,
        RouteRandomly:      false,
        
        // æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
        PoolSize:           100,
        PoolTimeout:        30 * time.Second,
        IdleTimeout:        5 * time.Minute,
        IdleCheckFrequency: 1 * time.Minute,
        
        // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        DialTimeout:  10 * time.Second,
        ReadTimeout:  5 * time.Second,
        WriteTimeout: 5 * time.Second,
    })
    
    // æ¥ç¶šãƒ†ã‚¹ãƒˆ
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    if err := cluster.Ping(ctx).Err(); err != nil {
        return nil, fmt.Errorf("failed to connect to Redis cluster: %w", err)
    }
    
    hashRing := NewConsistentHash(3, nodes)
    nodeManager := &NodeManager{
        nodes:       nodes,
        healthCheck: make(map[string]bool),
    }
    
    // å…¨ãƒãƒ¼ãƒ‰ã‚’åˆæœŸçŠ¶æ…‹ã§ã¯å¥å…¨ã¨ãƒãƒ¼ã‚¯
    for _, node := range nodes {
        nodeManager.healthCheck[node] = true
    }
    
    ccm := &ClusterCacheManager{
        cluster:     cluster,
        hashRing:    hashRing,
        nodeManager: nodeManager,
        metrics: &ClusterMetrics{
            NodeLatency: make(map[string]time.Duration),
            NodeLoad:    make(map[string]int64),
        },
    }
    
    // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹
    go ccm.startHealthMonitoring()
    
    return ccm, nil
}

func (ccm *ClusterCacheManager) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    data, err := json.Marshal(value)
    if err != nil {
        return fmt.Errorf("failed to marshal value: %w", err)
    }
    
    start := time.Now()
    err = ccm.cluster.Set(ctx, key, data, ttl).Err()
    
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    node := ccm.hashRing.GetNode(key)
    ccm.recordNodeLatency(node, time.Since(start))
    
    if err != nil {
        ccm.recordClusterError()
        return fmt.Errorf("failed to set cache: %w", err)
    }
    
    return nil
}

func (ccm *ClusterCacheManager) Get(ctx context.Context, key string) (interface{}, error) {
    start := time.Now()
    result, err := ccm.cluster.Get(ctx, key).Result()
    
    node := ccm.hashRing.GetNode(key)
    ccm.recordNodeLatency(node, time.Since(start))
    
    if err == redis.Nil {
        return nil, ErrCacheMiss
    }
    
    if err != nil {
        ccm.recordClusterError()
        return nil, fmt.Errorf("failed to get cache: %w", err)
    }
    
    var data interface{}
    if err := json.Unmarshal([]byte(result), &data); err != nil {
        return nil, fmt.Errorf("failed to unmarshal cached data: %w", err)
    }
    
    return data, nil
}

func (ccm *ClusterCacheManager) startHealthMonitoring() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        ccm.checkClusterHealth()
    }
}

func (ccm *ClusterCacheManager) checkClusterHealth() {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—
    clusterNodes, err := ccm.cluster.ClusterNodes(ctx).Result()
    if err != nil {
        log.Printf("Failed to get cluster nodes: %v", err)
        return
    }
    
    ccm.updateNodeHealth(clusterNodes)
}

func (ccm *ClusterCacheManager) updateNodeHealth(nodesInfo string) {
    ccm.nodeManager.mu.Lock()
    defer ccm.nodeManager.mu.Unlock()
    
    lines := strings.Split(nodesInfo, "\n")
    for _, line := range lines {
        if line == "" {
            continue
        }
        
        parts := strings.Fields(line)
        if len(parts) < 8 {
            continue
        }
        
        nodeID := parts[0]
        address := strings.Split(parts[1], "@")[0] // Remove port for cluster
        flags := parts[2]
        
        // ãƒãƒ¼ãƒ‰ã®å¥å…¨æ€§ã‚’ãƒ•ãƒ©ã‚°ã‹ã‚‰åˆ¤å®š
        isHealthy := !strings.Contains(flags, "fail") && 
                    !strings.Contains(flags, "handshake") &&
                    (strings.Contains(flags, "master") || strings.Contains(flags, "slave"))
        
        ccm.nodeManager.healthCheck[address] = isHealthy
        
        log.Printf("Node %s (%s): healthy=%v", nodeID[:8], address, isHealthy)
    }
}

func (ccm *ClusterCacheManager) recordNodeLatency(node string, latency time.Duration) {
    ccm.metrics.mu.Lock()
    defer ccm.metrics.mu.Unlock()
    ccm.metrics.NodeLatency[node] = latency
}

func (ccm *ClusterCacheManager) recordClusterError() {
    ccm.metrics.mu.Lock()
    defer ccm.metrics.mu.Unlock()
    ccm.metrics.ClusterErrors++
}

func NewConsistentHash(replicas int, nodes []string) *ConsistentHash {
    ch := &ConsistentHash{
        nodes:    make(map[uint32]string),
        replicas: replicas,
    }
    
    for _, node := range nodes {
        ch.AddNode(node)
    }
    
    return ch
}

func (ch *ConsistentHash) AddNode(node string) {
    ch.mu.Lock()
    defer ch.mu.Unlock()
    
    for i := 0; i < ch.replicas; i++ {
        hash := ch.hashKey(fmt.Sprintf("%s:%d", node, i))
        ch.nodes[hash] = node
        ch.keys = append(ch.keys, hash)
    }
    
    sort.Slice(ch.keys, func(i, j int) bool {
        return ch.keys[i] < ch.keys[j]
    })
}

func (ch *ConsistentHash) GetNode(key string) string {
    ch.mu.RLock()
    defer ch.mu.RUnlock()
    
    if len(ch.keys) == 0 {
        return ""
    }
    
    hash := ch.hashKey(key)
    
    // Find the first node with hash >= key hash
    idx := sort.Search(len(ch.keys), func(i int) bool {
        return ch.keys[i] >= hash
    })
    
    // Wrap around if necessary
    if idx == len(ch.keys) {
        idx = 0
    }
    
    return ch.nodes[ch.keys[idx]]
}

func (ch *ConsistentHash) hashKey(key string) uint32 {
    h := fnv.New32a()
    h.Write([]byte(key))
    return h.Sum32()
}
```

ğŸ“ **èª²é¡Œ**

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤Redisé«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

1. **`CacheManager`**: åŸºæœ¬çš„ãªRedisã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ
2. **`MultiLayerCache`**: L1/L2å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
3. **`BatchCacheManager`**: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã¨ãƒãƒƒãƒæ“ä½œ
4. **`ClusterCacheManager`**: Redis Clusteråˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥
5. **`CacheMetrics`**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
6. **çµ±åˆãƒ†ã‚¹ãƒˆ**: å®Ÿéš›ã®Redisç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

âœ… **æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹•**

å®Ÿè£…ãŒå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‹•ä½œãŒæœŸå¾…ã•ã‚Œã¾ã™ï¼š

```bash
$ go test -v
=== RUN   TestCacheManager_BasicOperations
--- PASS: TestCacheManager_BasicOperations (0.05s)
=== RUN   TestMultiLayerCache_L1L2Performance
--- PASS: TestMultiLayerCache_L1L2Performance (0.08s)
=== RUN   TestBatchCacheManager_PipelineOps
--- PASS: TestBatchCacheManager_PipelineOps (0.10s)
=== RUN   TestClusterCacheManager_DistributedOps
--- PASS: TestClusterCacheManager_DistributedOps (0.15s)
=== RUN   TestCacheMetrics_PerformanceTracking
--- PASS: TestCacheMetrics_PerformanceTracking (0.12s)
PASS
ok      day41-redis-caching    0.500s
```

ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

1. **go-redis/redis**: æœ€æ–°ã®Redis Go ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
2. **Redis Pipeline**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š
3. **Consistent Hashing**: åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ã®å‡ç­‰ãªè² è·åˆ†æ•£
4. **TTL Management**: é©åˆ‡ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™ç®¡ç†
5. **Error Handling**: Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†

è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š
- **æ¥ç¶šãƒ—ãƒ¼ãƒ«**: Redisæ¥ç¶šã®åŠ¹ç‡çš„ãªç®¡ç†
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥**: Write-Through, Write-Behind, Cache-Aside
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: JSON vs MessagePack vs Protocol Buffers
- **ç›£è¦–**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ãƒ’ãƒƒãƒˆç‡ã€ã‚¨ãƒ©ãƒ¼ç‡ã®è¿½è·¡

## å®Ÿè¡Œæ–¹æ³•

```bash
go test -v
go test -race  # ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã®æ¤œå‡º
go test -bench=.  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
```
})
```

### TTL (Time To Live) ç®¡ç†

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã«ã¯é©åˆ‡ãªæœ‰åŠ¹æœŸé™ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ï¼š

```go
// TTL ä»˜ãã§ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
rdb.Set(ctx, "session:12345", userData, 30*time.Minute)

// TTL ã‚’ç¢ºèª
ttl := rdb.TTL(ctx, "session:12345").Val()

// TTL ã‚’æ›´æ–°
rdb.Expire(ctx, "session:12345", time.Hour)
```

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

Redis ã®æ¥ç¶šçŠ¶æ…‹ã‚’ç›£è¦–ã™ã‚‹ã“ã¨ã¯é‡è¦ã§ã™ï¼š

```go
// Ping ã§ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
pong, err := rdb.Ping(ctx).Result()
if err != nil {
    log.Printf("Redis connection failed: %v", err)
}
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

Redis ç‰¹æœ‰ã®ã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

```go
val, err := rdb.Get(ctx, "key").Result()
if err == redis.Nil {
    // ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆ
    fmt.Println("Key does not exist")
} else if err != nil {
    // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
    log.Printf("Redis error: %v", err)
}
```

## ğŸ“ èª²é¡Œ (The Problem)

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤ Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

### 1. CacheClient æ§‹é€ ä½“ã®å®Ÿè£…

```go
type CacheClient struct {
    client *redis.Client
    stats  *CacheStats
}

type CacheStats struct {
    Hits   int64
    Misses int64
    Errors int64
}
```

### 2. å¿…è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…

- `NewCacheClient(addr string) (*CacheClient, error)`: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
- `Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error`: ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
- `Get(ctx context.Context, key string) (string, error)`: ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
- `Delete(ctx context.Context, key string) error`: ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
- `Exists(ctx context.Context, key string) (bool, error)`: ã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
- `GetStats() CacheStats`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã®å–å¾—
- `HealthCheck(ctx context.Context) error`: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- `Close() error`: æ¥ç¶šã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### 3. çµ±è¨ˆæƒ…å ±ã®ç®¡ç†

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ’ãƒƒãƒˆç‡ã€ãƒŸã‚¹ç‡ã€ã‚¨ãƒ©ãƒ¼ç‡ã‚’è¿½è·¡ã—ã¦ãã ã•ã„ã€‚

## âœ… æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹• (Expected Behavior)

å®Ÿè£…ãŒæ­£ã—ãå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ†ã‚¹ãƒˆçµæœãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

```bash
$ go test -v
=== RUN   TestCacheClient_BasicOperations
    main_test.go:45: Set operation successful
    main_test.go:52: Retrieved value: test_value
    main_test.go:59: Key exists: true
    main_test.go:66: Key deleted successfully
    main_test.go:73: Key no longer exists: false
--- PASS: TestCacheClient_BasicOperations (0.02s)

=== RUN   TestCacheClient_TTL
    main_test.go:95: Value set with TTL
    main_test.go:102: Value retrieved before expiration: ttl_value
    main_test.go:109: Value expired and no longer accessible
--- PASS: TestCacheClient_TTL (1.51s)

=== RUN   TestCacheClient_Stats
    main_test.go:135: Cache stats - Hits: 2, Misses: 1, Errors: 0
--- PASS: TestCacheClient_Stats (0.01s)

=== RUN   TestCacheClient_HealthCheck
    main_test.go:150: Health check passed
--- PASS: TestCacheClient_HealthCheck (0.01s)

PASS
ok      day41-redis-caching     1.672s
```

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ (Hints)

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã®ãƒ’ãƒ³ãƒˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

### ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```go
import (
    "context"
    "encoding/json"
    "sync/atomic"
    "time"
    
    "github.com/go-redis/redis/v8"
)
```

### ä¾å­˜é–¢ä¿‚

```bash
go mod init day41-redis-caching
go get github.com/go-redis/redis/v8
go get github.com/ory/dockertest/v3
```

### çµ±è¨ˆæƒ…å ±ã®åŸå­çš„æ“ä½œ

```go
// ãƒ’ãƒƒãƒˆæ•°ã®å¢—åŠ 
atomic.AddInt64(&c.stats.Hits, 1)

// ãƒŸã‚¹æ•°ã®å¢—åŠ 
atomic.AddInt64(&c.stats.Misses, 1)
```

### JSON ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆï¼š

```go
data, err := json.Marshal(value)
if err != nil {
    return err
}
return c.client.Set(ctx, key, data, ttl).Err()
```

### ã‚¨ãƒ©ãƒ¼åˆ†é¡

```go
if err == redis.Nil {
    // ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„
    atomic.AddInt64(&c.stats.Misses, 1)
    return "", ErrCacheMiss
} else if err != nil {
    // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
    atomic.AddInt64(&c.stats.Errors, 1)
    return "", err
}
```

### Docker ãƒ†ã‚¹ãƒˆç’°å¢ƒ

ãƒ†ã‚¹ãƒˆã§ Redis ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼š

```go
func setupRedis(t *testing.T) (*redis.Client, func()) {
    pool, err := dockertest.NewPool("")
    require.NoError(t, err)
    
    resource, err := pool.Run("redis", "7", nil)
    require.NoError(t, err)
    
    // æ¥ç¶šç¢ºèªã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°ã‚’è¿”ã™
}
```

## ğŸš€ ç™ºå±•èª²é¡Œ (Advanced Challenges)

åŸºæœ¬å®Ÿè£…ãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ç™ºå±•çš„ãªæ©Ÿèƒ½ã«ã‚‚ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¦ã¿ã¦ãã ã•ã„ï¼š

1. **ãƒãƒ«ã‚¯æ“ä½œ**: è¤‡æ•°ã®ã‚­ãƒ¼ã‚’ä¸€åº¦ã«æ“ä½œã™ã‚‹æ©Ÿèƒ½
2. **ã‚­ãƒ¼åå‰ç©ºé–“**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ¥ã«ã‚­ãƒ¼ã‚’åˆ†é›¢ã™ã‚‹æ©Ÿèƒ½
3. **åœ§ç¸®**: å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹æ©Ÿèƒ½
4. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å‡ºåŠ›
5. **ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼**: Redis ã‚µãƒ¼ãƒãƒ¼ãƒ€ã‚¦ãƒ³æ™‚ã®å¯¾å‡¦

å®Ÿè£…ã‚’é€šã˜ã¦ã€Redis ã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã¨ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­è¨ˆã®åŸºç¤ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼
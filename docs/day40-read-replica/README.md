# Day 40: Read-Replicaåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã¨é«˜å¯ç”¨æ€§DBè¨­è¨ˆ

## ğŸ¯ æœ¬æ—¥ã®ç›®æ¨™

ã“ã®ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’é€šã—ã¦ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

- **Read-Replicaã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªè² è·åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹**
- **Master/Slaveæ§‹æˆã§ã®è‡ªå‹•ãƒ•ã‚§ãƒ¼ãƒ«ã‚ªãƒ¼ãƒãƒ¼æ©Ÿèƒ½ã‚’å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹**
- **ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’è€ƒæ…®ã—ãŸå …ç‰¢ãªãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç®¡ç†ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹**
- **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®Read-Replicaé‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¿’å¾—ã™ã‚‹**

## ğŸ“– è§£èª¬

### ãªãœRead-ReplicaãŒå¿…è¦ãªã®ã‹ï¼Ÿ

ç¾ä»£ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€èª­ã¿å–ã‚Šæ“ä½œãŒæ›¸ãè¾¼ã¿æ“ä½œã®10å€ä»¥ä¸Šç™ºç”Ÿã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã¯ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š

#### å˜ä¸€DBæ§‹æˆã®é™ç•Œ

```go
// å•é¡Œã®ã‚ã‚‹ä¾‹ï¼šå…¨ã¦ã®ã‚¯ã‚¨ãƒªãŒä¸€ã¤ã®DBã«é›†ä¸­
func GetUserDashboard(db *sql.DB, userID int) (*Dashboard, error) {
    // ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªãŒå…¨ã¦åŒä¸€DBã§å®Ÿè¡Œã•ã‚Œã‚‹
    
    // 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±
    user, err := getUserBasicInfo(db, userID)
    if err != nil {
        return nil, err
    }
    
    // 2. æœ€è¿‘ã®æ³¨æ–‡å±¥æ­´ï¼ˆè¤‡é›‘ãªJOINï¼‰
    orders, err := getRecentOrders(db, userID)
    if err != nil {
        return nil, err
    }
    
    // 3. æ¨å¥¨å•†å“ï¼ˆé‡ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
    recommendations, err := getRecommendations(db, userID)
    if err != nil {
        return nil, err
    }
    
    // 4. çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆé›†è¨ˆã‚¯ã‚¨ãƒªï¼‰
    stats, err := getUserStats(db, userID)
    if err != nil {
        return nil, err
    }
    
    return &Dashboard{
        User:            user,
        RecentOrders:    orders,
        Recommendations: recommendations,
        Stats:          stats,
    }, nil
}

func getRecommendations(db *sql.DB, userID int) ([]Product, error) {
    // éå¸¸ã«é‡ã„é›†è¨ˆã‚¯ã‚¨ãƒªã®ä¾‹
    query := `
        WITH user_preferences AS (
            SELECT category_id, COUNT(*) as purchase_count
            FROM orders o
            JOIN order_items oi ON o.id = oi.order_id
            JOIN products p ON oi.product_id = p.id
            WHERE o.user_id = $1
            GROUP BY category_id
        ),
        similar_users AS (
            SELECT DISTINCT o2.user_id, 
                   COUNT(*) as similarity_score
            FROM orders o1
            JOIN orders o2 ON o1.product_id = o2.product_id
            WHERE o1.user_id = $1 AND o2.user_id != $1
            GROUP BY o2.user_id
            HAVING COUNT(*) >= 3
            ORDER BY similarity_score DESC
            LIMIT 100
        )
        SELECT p.id, p.name, p.price, AVG(r.rating) as avg_rating
        FROM products p
        JOIN order_items oi ON p.id = oi.product_id
        JOIN orders o ON oi.order_id = o.id
        JOIN similar_users su ON o.user_id = su.user_id
        LEFT JOIN reviews r ON p.id = r.product_id
        WHERE p.category_id IN (SELECT category_id FROM user_preferences)
        GROUP BY p.id, p.name, p.price
        HAVING COUNT(DISTINCT o.user_id) >= 5
        ORDER BY avg_rating DESC, COUNT(DISTINCT o.user_id) DESC
        LIMIT 10
    `
    
    // ã“ã®ã‚¯ã‚¨ãƒªãŒ5ç§’ã‹ã‹ã‚‹ã¨ã€ä»–ã®è»½ã„ã‚¯ã‚¨ãƒªã‚‚å½±éŸ¿ã‚’å—ã‘ã‚‹
    rows, err := db.Query(query, userID)
    // ...
}
```

**å•é¡Œç‚¹ï¼š**
- **ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆ**: é‡ã„ã‚¯ã‚¨ãƒªãŒè»½ã„ã‚¯ã‚¨ãƒªã‚’ãƒ–ãƒ­ãƒƒã‚¯
- **å˜ä¸€éšœå®³ç‚¹**: DBãŒãƒ€ã‚¦ãƒ³ã™ã‚‹ã¨ã‚µãƒ¼ãƒ“ã‚¹å…¨ä½“ãŒåœæ­¢
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œ**: å‚ç›´ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ã¿ã§ã‚³ã‚¹ãƒˆãŒå¢—å¤§
- **åœ°ç†çš„åˆ¶ç´„**: å…¨ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŒä¸€æ‹ ç‚¹ã®DBã«ã‚¢ã‚¯ã‚»ã‚¹

### Read-Replicaã«ã‚ˆã‚‹åŠ‡çš„ãªæ”¹å–„

Read-Replicaï¼ˆèª­ã¿å–ã‚Šãƒ¬ãƒ—ãƒªã‚«ï¼‰ã¯ã€Master/Primary DBã‹ã‚‰éåŒæœŸã§ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡è£½ã—ã€èª­ã¿å–ã‚Šå°‚ç”¨æ“ä½œã‚’åˆ†æ•£ã™ã‚‹ä»•çµ„ã¿ã§ã™ï¼š

```go
type DatabaseCluster struct {
    master   *sql.DB
    replicas []*sql.DB
    
    // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
    replicaHealth   map[int]bool
    healthMutex     sync.RWMutex
    
    // ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼
    loadBalancer    ReplicaLoadBalancer
    
    // è¨­å®š
    config          ClusterConfig
    
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metrics         *ClusterMetrics
}

type ClusterConfig struct {
    MaxRetries           int
    HealthCheckInterval  time.Duration
    ReplicationLagThreshold time.Duration
    PreferLocalReplica   bool
    Region              string
}

type ClusterMetrics struct {
    QueriesTotal        map[string]int64 // master/replicaåˆ¥
    QueryDuration       map[string]time.Duration
    FailoverCount       int64
    ReplicationLag      map[int]time.Duration
    mu                  sync.RWMutex
}

func NewDatabaseCluster(masterDSN string, replicaDSNs []string, config ClusterConfig) (*DatabaseCluster, error) {
    // Masteræ¥ç¶š
    master, err := sql.Open("postgres", masterDSN)
    if err != nil {
        return nil, fmt.Errorf("failed to connect to master: %w", err)
    }
    
    // Replicaæ¥ç¶š
    replicas := make([]*sql.DB, len(replicaDSNs))
    replicaHealth := make(map[int]bool)
    
    for i, dsn := range replicaDSNs {
        replica, err := sql.Open("postgres", dsn)
        if err != nil {
            // ä»–ã®ãƒ¬ãƒ—ãƒªã‚«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ç¶™ç¶š
            log.Printf("Failed to connect to replica %d: %v", i, err)
            replicaHealth[i] = false
            continue
        }
        
        replicas[i] = replica
        replicaHealth[i] = true
    }
    
    cluster := &DatabaseCluster{
        master:        master,
        replicas:      replicas,
        replicaHealth: replicaHealth,
        config:        config,
        metrics:       &ClusterMetrics{
            QueriesTotal:  make(map[string]int64),
            QueryDuration: make(map[string]time.Duration),
            ReplicationLag: make(map[int]time.Duration),
        },
        loadBalancer:  NewWeightedRoundRobinBalancer(),
    }
    
    // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹
    go cluster.startHealthChecking()
    
    return cluster, nil
}
```

**æ”¹å–„åŠ¹æœï¼š**
- **è² è·åˆ†æ•£**: èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªãŒãƒ¬ãƒ—ãƒªã‚«ã«åˆ†æ•£ã•ã‚Œã€Masterã®è² è·ãŒ80%å‰Šæ¸›
- **é«˜å¯ç”¨æ€§**: ãƒ¬ãƒ—ãƒªã‚«éšœå®³æ™‚ã‚‚ä»–ã®ãƒ¬ãƒ—ãƒªã‚«ã§ç¶™ç¶šã‚µãƒ¼ãƒ“ã‚¹
- **åœ°ç†çš„åˆ†æ•£**: å„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ¬ãƒ—ãƒªã‚«ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›
- **æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: èª­ã¿å–ã‚Šæ€§èƒ½ã‚’ãƒ¬ãƒ—ãƒªã‚«è¿½åŠ ã§ç·šå½¢æ‹¡å¼µ

### é«˜åº¦ãªRead-Replicaç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

#### 1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¯ã‚¨ãƒªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

```go
type QueryRouter struct {
    cluster      *DatabaseCluster
    analyzer     *QueryAnalyzer
    ruleEngine   *RoutingRuleEngine
}

type QueryAnalyzer struct {
    readPatterns  []string
    writePatterns []string
    heavyPatterns []string
}

func NewQueryAnalyzer() *QueryAnalyzer {
    return &QueryAnalyzer{
        readPatterns: []string{
            `^SELECT\s+`,
            `^WITH\s+.+\s+SELECT\s+`,
            `^EXPLAIN\s+`,
        },
        writePatterns: []string{
            `^INSERT\s+`,
            `^UPDATE\s+`,
            `^DELETE\s+`,
            `^CREATE\s+`,
            `^DROP\s+`,
            `^ALTER\s+`,
        },
        heavyPatterns: []string{
            `JOIN\s+.*JOIN\s+.*JOIN`, // è¤‡æ•°JOINã¯é‡ã„ã‚¯ã‚¨ãƒª
            `GROUP\s+BY\s+.*HAVING`,   // é›†è¨ˆã‚¯ã‚¨ãƒª
            `ORDER\s+BY\s+.*LIMIT\s+\d{3,}`, // å¤§é‡ã‚½ãƒ¼ãƒˆ
            `COUNT\(\*\).*FROM\s+\w+\s*$`, // å…¨ä»¶COUNT
        },
    }
}

func (qa *QueryAnalyzer) AnalyzeQuery(query string) QueryType {
    normalizedQuery := strings.ToUpper(strings.TrimSpace(query))
    
    // æ›¸ãè¾¼ã¿ã‚¯ã‚¨ãƒªã®åˆ¤å®š
    for _, pattern := range qa.writePatterns {
        if matched, _ := regexp.MatchString(pattern, normalizedQuery); matched {
            return QueryTypeWrite
        }
    }
    
    // é‡ã„ã‚¯ã‚¨ãƒªã®åˆ¤å®š
    for _, pattern := range qa.heavyPatterns {
        if matched, _ := regexp.MatchString(pattern, normalizedQuery); matched {
            return QueryTypeHeavyRead
        }
    }
    
    // èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªã®åˆ¤å®š
    for _, pattern := range qa.readPatterns {
        if matched, _ := regexp.MatchString(pattern, normalizedQuery); matched {
            return QueryTypeLightRead
        }
    }
    
    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ›¸ãè¾¼ã¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨å´ï¼‰
    return QueryTypeWrite
}

type QueryType int

const (
    QueryTypeWrite QueryType = iota
    QueryTypeLightRead
    QueryTypeHeavyRead
)

type RoutingRuleEngine struct {
    rules []RoutingRule
}

type RoutingRule struct {
    Condition func(QueryContext) bool
    Action    RoutingAction
    Priority  int
}

type QueryContext struct {
    Query           string
    QueryType       QueryType
    UserID          int
    RequiredConsistency ConsistencyLevel
    Timeout         time.Duration
    Tags            map[string]string
}

type RoutingAction struct {
    TargetType    DatabaseTargetType
    MaxLag        time.Duration
    Fallback      DatabaseTargetType
    StickySessions bool
}

type DatabaseTargetType int

const (
    TargetMaster DatabaseTargetType = iota
    TargetAnyReplica
    TargetLocalReplica
    TargetSpecificReplica
)

type ConsistencyLevel int

const (
    ConsistencyEventual ConsistencyLevel = iota
    ConsistencyReadAfterWrite
    ConsistencyStrong
)

func (qr *QueryRouter) RouteQuery(ctx context.Context, queryCtx QueryContext) (*sql.DB, error) {
    // ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ã§æœ€é©ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ±ºå®š
    action := qr.ruleEngine.EvaluateRules(queryCtx)
    
    switch action.TargetType {
    case TargetMaster:
        return qr.cluster.master, nil
        
    case TargetAnyReplica:
        return qr.selectHealthyReplica(ctx, action.MaxLag)
        
    case TargetLocalReplica:
        return qr.selectLocalReplica(ctx, action.MaxLag)
        
    default:
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if action.Fallback == TargetMaster {
            return qr.cluster.master, nil
        }
        return qr.selectHealthyReplica(ctx, action.MaxLag)
    }
}

func (qr *QueryRouter) selectHealthyReplica(ctx context.Context, maxLag time.Duration) (*sql.DB, error) {
    qr.cluster.healthMutex.RLock()
    defer qr.cluster.healthMutex.RUnlock()
    
    var candidates []ReplicaCandidate
    
    for i, replica := range qr.cluster.replicas {
        if !qr.cluster.replicaHealth[i] {
            continue
        }
        
        // ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’ãƒã‚§ãƒƒã‚¯
        lag, exists := qr.cluster.metrics.ReplicationLag[i]
        if exists && lag > maxLag {
            continue
        }
        
        candidates = append(candidates, ReplicaCandidate{
            Index:    i,
            Database: replica,
            Lag:      lag,
            Weight:   qr.calculateReplicaWeight(i),
        })
    }
    
    if len(candidates) == 0 {
        // åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ—ãƒªã‚«ãŒãªã„å ´åˆã¯ãƒã‚¹ã‚¿ãƒ¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        qr.cluster.metrics.mu.Lock()
        qr.cluster.metrics.FailoverCount++
        qr.cluster.metrics.mu.Unlock()
        
        return qr.cluster.master, nil
    }
    
    // ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ã§æœ€é©ãªãƒ¬ãƒ—ãƒªã‚«ã‚’é¸æŠ
    selected := qr.cluster.loadBalancer.SelectReplica(candidates)
    return selected.Database, nil
}

type ReplicaCandidate struct {
    Index    int
    Database *sql.DB
    Lag      time.Duration
    Weight   float64
}

func (qr *QueryRouter) calculateReplicaWeight(replicaIndex int) float64 {
    // CPUä½¿ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãªã©ã‚’è€ƒæ…®
    baseWeight := 1.0
    
    // ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã«ã‚ˆã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£
    if lag, exists := qr.cluster.metrics.ReplicationLag[replicaIndex]; exists {
        lagPenalty := float64(lag.Milliseconds()) / 1000.0
        baseWeight -= lagPenalty * 0.1
    }
    
    // ã‚¯ã‚¨ãƒªè² è·ã«ã‚ˆã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£
    if qr.cluster.metrics.QueriesTotal["replica_"+string(rune(replicaIndex))] > 1000 {
        baseWeight -= 0.2
    }
    
    return math.Max(0.1, baseWeight)
}
```

#### 2. ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ç›£è¦–ã¨è‡ªå‹•èª¿æ•´

```go
type ReplicationLagMonitor struct {
    cluster       *DatabaseCluster
    ticker        *time.Ticker
    done          chan struct{}
    alertManager  *AlertManager
}

func NewReplicationLagMonitor(cluster *DatabaseCluster, interval time.Duration) *ReplicationLagMonitor {
    return &ReplicationLagMonitor{
        cluster:      cluster,
        ticker:       time.NewTicker(interval),
        done:         make(chan struct{}),
        alertManager: NewAlertManager(),
    }
}

func (rlm *ReplicationLagMonitor) Start() {
    go func() {
        defer rlm.ticker.Stop()
        for {
            select {
            case <-rlm.ticker.C:
                rlm.checkReplicationLag()
            case <-rlm.done:
                return
            }
        }
    }()
}

func (rlm *ReplicationLagMonitor) checkReplicationLag() {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    // ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ç¾åœ¨ã®LSNã‚’å–å¾—
    var masterLSN string
    err := rlm.cluster.master.QueryRowContext(ctx, 
        "SELECT pg_current_wal_lsn()").Scan(&masterLSN)
    if err != nil {
        log.Printf("Failed to get master LSN: %v", err)
        return
    }
    
    // å„ãƒ¬ãƒ—ãƒªã‚«ã®é…å»¶ã‚’æ¸¬å®š
    for i, replica := range rlm.cluster.replicas {
        if replica == nil {
            continue
        }
        
        lag, err := rlm.measureLag(ctx, replica, masterLSN)
        if err != nil {
            log.Printf("Failed to measure lag for replica %d: %v", i, err)
            rlm.markReplicaUnhealthy(i)
            continue
        }
        
        // é…å»¶æƒ…å ±ã‚’æ›´æ–°
        rlm.cluster.metrics.mu.Lock()
        rlm.cluster.metrics.ReplicationLag[i] = lag
        rlm.cluster.metrics.mu.Unlock()
        
        // é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if lag > rlm.cluster.config.ReplicationLagThreshold {
            rlm.alertManager.SendAlert(Alert{
                Level:   "WARNING",
                Message: fmt.Sprintf("Replica %d lag: %v", i, lag),
                Time:    time.Now(),
            })
            
            // ä¸€æ™‚çš„ã«ãƒ¬ãƒ—ãƒªã‚«ã‚’ç„¡åŠ¹åŒ–
            rlm.markReplicaUnhealthy(i)
        } else {
            rlm.markReplicaHealthy(i)
        }
    }
}

func (rlm *ReplicationLagMonitor) measureLag(ctx context.Context, replica *sql.DB, masterLSN string) (time.Duration, error) {
    var replicaLSN string
    var lagBytes int64
    
    query := `
        SELECT 
            pg_last_wal_receive_lsn(),
            pg_wal_lsn_diff($1, pg_last_wal_replay_lsn())
    `
    
    err := replica.QueryRowContext(ctx, query, masterLSN).Scan(&replicaLSN, &lagBytes)
    if err != nil {
        return 0, err
    }
    
    // ãƒã‚¤ãƒˆæ•°ã‚’æ™‚é–“ã«å¤‰æ›ï¼ˆæ¦‚ç®—ï¼‰
    // å¹³å‡çš„ãªWALç”Ÿæˆé€Ÿåº¦ã‚’è€ƒæ…®
    avgWALSpeed := int64(1024 * 1024) // 1MB/sec
    lagSeconds := lagBytes / avgWALSpeed
    
    return time.Duration(lagSeconds) * time.Second, nil
}

func (rlm *ReplicationLagMonitor) markReplicaUnhealthy(index int) {
    rlm.cluster.healthMutex.Lock()
    defer rlm.cluster.healthMutex.Unlock()
    rlm.cluster.replicaHealth[index] = false
}

func (rlm *ReplicationLagMonitor) markReplicaHealthy(index int) {
    rlm.cluster.healthMutex.Lock()
    defer rlm.cluster.healthMutex.Unlock()
    rlm.cluster.replicaHealth[index] = true
}
```

#### 3. èª­ã¿å–ã‚Šå¾Œæ›¸ãè¾¼ã¿æ•´åˆæ€§ã®ä¿è¨¼

```go
type ConsistentDBManager struct {
    cluster         *DatabaseCluster
    router          *QueryRouter
    sessionManager  *SessionManager
}

type SessionManager struct {
    sessions map[string]*Session
    mu       sync.RWMutex
    ttl      time.Duration
}

type Session struct {
    UserID           int
    LastWriteTime    time.Time
    PreferredReplica int
    StickUntil       time.Time
}

func (cm *ConsistentDBManager) ExecuteWithConsistency(
    ctx context.Context, 
    sessionID string, 
    query string, 
    consistency ConsistencyLevel,
    args ...interface{},
) (*sql.Rows, error) {
    
    queryType := cm.router.analyzer.AnalyzeQuery(query)
    
    switch consistency {
    case ConsistencyStrong:
        // å¼·ã„æ•´åˆæ€§ãŒå¿…è¦ãªå ´åˆã¯å¸¸ã«ãƒã‚¹ã‚¿ãƒ¼
        return cm.cluster.master.QueryContext(ctx, query, args...)
        
    case ConsistencyReadAfterWrite:
        // æ›¸ãè¾¼ã¿å¾Œèª­ã¿å–ã‚Šæ•´åˆæ€§
        return cm.handleReadAfterWrite(ctx, sessionID, queryType, query, args...)
        
    case ConsistencyEventual:
        // çµæœæ•´åˆæ€§ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        return cm.handleEventualConsistency(ctx, queryType, query, args...)
        
    default:
        return nil, fmt.Errorf("unknown consistency level: %v", consistency)
    }
}

func (cm *ConsistentDBManager) handleReadAfterWrite(
    ctx context.Context, 
    sessionID string, 
    queryType QueryType, 
    query string, 
    args ...interface{},
) (*sql.Rows, error) {
    
    session := cm.sessionManager.GetSession(sessionID)
    
    if queryType == QueryTypeWrite {
        // æ›¸ãè¾¼ã¿ã¯å¸¸ã«ãƒã‚¹ã‚¿ãƒ¼
        rows, err := cm.cluster.master.QueryContext(ctx, query, args...)
        if err != nil {
            return nil, err
        }
        
        // ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ›´æ–°
        cm.sessionManager.UpdateSession(sessionID, Session{
            UserID:        session.UserID,
            LastWriteTime: time.Now(),
            StickUntil:    time.Now().Add(10 * time.Second), // 10ç§’é–“ã¯ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰èª­ã¿å–ã‚Š
        })
        
        return rows, nil
    }
    
    // èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªã®å ´åˆ
    if session != nil && time.Now().Before(session.StickUntil) {
        // æœ€è¿‘æ›¸ãè¾¼ã¿ã‚’è¡Œã£ãŸå ´åˆã¯ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰èª­ã¿å–ã‚Š
        return cm.cluster.master.QueryContext(ctx, query, args...)
    }
    
    // é€šå¸¸ã®ãƒ¬ãƒ—ãƒªã‚«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    queryCtx := QueryContext{
        Query:               query,
        QueryType:           queryType,
        RequiredConsistency: ConsistencyEventual,
        Timeout:             5 * time.Second,
    }
    
    db, err := cm.router.RouteQuery(ctx, queryCtx)
    if err != nil {
        return nil, err
    }
    
    return db.QueryContext(ctx, query, args...)
}

func (cm *ConsistentDBManager) handleEventualConsistency(
    ctx context.Context, 
    queryType QueryType, 
    query string, 
    args ...interface{},
) (*sql.Rows, error) {
    
    if queryType == QueryTypeWrite {
        return cm.cluster.master.QueryContext(ctx, query, args...)
    }
    
    // èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªã¯ãƒ¬ãƒ—ãƒªã‚«ã«åˆ†æ•£
    queryCtx := QueryContext{
        Query:               query,
        QueryType:           queryType,
        RequiredConsistency: ConsistencyEventual,
        Timeout:             5 * time.Second,
    }
    
    db, err := cm.router.RouteQuery(ctx, queryCtx)
    if err != nil {
        return nil, err
    }
    
    return db.QueryContext(ctx, query, args...)
}

func (sm *SessionManager) GetSession(sessionID string) *Session {
    sm.mu.RLock()
    defer sm.mu.RUnlock()
    
    session, exists := sm.sessions[sessionID]
    if !exists {
        return &Session{}
    }
    
    // TTLãƒã‚§ãƒƒã‚¯
    if time.Since(session.LastWriteTime) > sm.ttl {
        delete(sm.sessions, sessionID)
        return &Session{}
    }
    
    return session
}

func (sm *SessionManager) UpdateSession(sessionID string, session Session) {
    sm.mu.Lock()
    defer sm.mu.Unlock()
    sm.sessions[sessionID] = &session
}
```

#### 4. ãƒ•ã‚§ãƒ¼ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã¨è‡ªå‹•å¾©æ—§

```go
type FailoverManager struct {
    cluster        *DatabaseCluster
    healthMonitor  *HealthMonitor
    alertManager   *AlertManager
    failoverPolicy FailoverPolicy
}

type FailoverPolicy struct {
    HealthCheckInterval   time.Duration
    FailureThreshold      int
    RecoveryThreshold     int
    AutoPromoteReplica    bool
    MaxPromotionAttempts  int
}

type HealthMonitor struct {
    cluster           *DatabaseCluster
    failures          map[int]int  // replica index -> failure count
    successes         map[int]int  // replica index -> success count
    lastHealthCheck   map[int]time.Time
    mu                sync.RWMutex
}

func (fm *FailoverManager) StartMonitoring() {
    go fm.monitorHealth()
}

func (fm *FailoverManager) monitorHealth() {
    ticker := time.NewTicker(fm.failoverPolicy.HealthCheckInterval)
    defer ticker.Stop()
    
    for range ticker.C {
        fm.checkAllDatabases()
    }
}

func (fm *FailoverManager) checkAllDatabases() {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    // ãƒã‚¹ã‚¿ãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    if err := fm.checkMasterHealth(ctx); err != nil {
        fm.handleMasterFailure(err)
    }
    
    // ãƒ¬ãƒ—ãƒªã‚«ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    for i, replica := range fm.cluster.replicas {
        if replica == nil {
            continue
        }
        
        if err := fm.checkReplicaHealth(ctx, i, replica); err != nil {
            fm.handleReplicaFailure(i, err)
        } else {
            fm.handleReplicaRecovery(i)
        }
    }
}

func (fm *FailoverManager) checkMasterHealth(ctx context.Context) error {
    var result int
    query := "SELECT 1"
    
    err := fm.cluster.master.QueryRowContext(ctx, query).Scan(&result)
    if err != nil {
        return fmt.Errorf("master health check failed: %w", err)
    }
    
    if result != 1 {
        return fmt.Errorf("master health check returned unexpected value: %d", result)
    }
    
    return nil
}

func (fm *FailoverManager) checkReplicaHealth(ctx context.Context, index int, replica *sql.DB) error {
    var result int
    query := "SELECT 1"
    
    err := replica.QueryRowContext(ctx, query).Scan(&result)
    if err != nil {
        return fmt.Errorf("replica %d health check failed: %w", index, err)
    }
    
    if result != 1 {
        return fmt.Errorf("replica %d health check returned unexpected value: %d", index, result)
    }
    
    return nil
}

func (fm *FailoverManager) handleMasterFailure(err error) {
    fm.alertManager.SendAlert(Alert{
        Level:   "CRITICAL",
        Message: fmt.Sprintf("Master database failure: %v", err),
        Time:    time.Now(),
    })
    
    if fm.failoverPolicy.AutoPromoteReplica {
        fm.attemptReplicaPromotion()
    }
}

func (fm *FailoverManager) handleReplicaFailure(index int, err error) {
    fm.healthMonitor.mu.Lock()
    defer fm.healthMonitor.mu.Unlock()
    
    fm.healthMonitor.failures[index]++
    fm.healthMonitor.successes[index] = 0
    
    if fm.healthMonitor.failures[index] >= fm.failoverPolicy.FailureThreshold {
        // ãƒ¬ãƒ—ãƒªã‚«ã‚’ç„¡åŠ¹åŒ–
        fm.cluster.healthMutex.Lock()
        fm.cluster.replicaHealth[index] = false
        fm.cluster.healthMutex.Unlock()
        
        fm.alertManager.SendAlert(Alert{
            Level:   "WARNING",
            Message: fmt.Sprintf("Replica %d marked as unhealthy after %d failures", 
                               index, fm.healthMonitor.failures[index]),
            Time:    time.Now(),
        })
    }
}

func (fm *FailoverManager) handleReplicaRecovery(index int) {
    fm.healthMonitor.mu.Lock()
    defer fm.healthMonitor.mu.Unlock()
    
    fm.healthMonitor.successes[index]++
    
    if fm.healthMonitor.successes[index] >= fm.failoverPolicy.RecoveryThreshold {
        // ãƒ¬ãƒ—ãƒªã‚«ã‚’å¾©æ—§
        fm.cluster.healthMutex.Lock()
        fm.cluster.replicaHealth[index] = true
        fm.cluster.healthMutex.Unlock()
        
        // ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
        fm.healthMonitor.failures[index] = 0
        
        fm.alertManager.SendAlert(Alert{
            Level:   "INFO",
            Message: fmt.Sprintf("Replica %d recovered and marked as healthy", index),
            Time:    time.Now(),
        })
    }
}

func (fm *FailoverManager) attemptReplicaPromotion() {
    // æœ€ã‚‚å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã‚’é¸æŠã—ã¦ãƒã‚¹ã‚¿ãƒ¼ã«æ˜‡æ ¼
    bestReplica := fm.selectBestReplicaForPromotion()
    if bestReplica == -1 {
        fm.alertManager.SendAlert(Alert{
            Level:   "CRITICAL",
            Message: "No healthy replica available for promotion",
            Time:    time.Now(),
        })
        return
    }
    
    // ãƒ¬ãƒ—ãƒªã‚«ã‚’ãƒã‚¹ã‚¿ãƒ¼ã«æ˜‡æ ¼ï¼ˆå®Ÿéš›ã®DBã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã«ã‚ˆã‚‹ï¼‰
    if err := fm.promoteReplica(bestReplica); err != nil {
        fm.alertManager.SendAlert(Alert{
            Level:   "CRITICAL",
            Message: fmt.Sprintf("Failed to promote replica %d: %v", bestReplica, err),
            Time:    time.Now(),
        })
        return
    }
    
    fm.alertManager.SendAlert(Alert{
        Level:   "INFO",
        Message: fmt.Sprintf("Successfully promoted replica %d to master", bestReplica),
        Time:    time.Now(),
    })
}

func (fm *FailoverManager) selectBestReplicaForPromotion() int {
    fm.healthMonitor.mu.RLock()
    defer fm.healthMonitor.mu.RUnlock()
    
    bestIndex := -1
    minLag := time.Hour // åˆæœŸå€¤ã¯ååˆ†å¤§ããªå€¤
    
    for i, replica := range fm.cluster.replicas {
        if replica == nil || !fm.cluster.replicaHealth[i] {
            continue
        }
        
        lag, exists := fm.cluster.metrics.ReplicationLag[i]
        if !exists {
            continue
        }
        
        if lag < minLag {
            minLag = lag
            bestIndex = i
        }
    }
    
    return bestIndex
}

func (fm *FailoverManager) promoteReplica(index int) error {
    // ã“ã®å®Ÿè£…ã¯PostgreSQLã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã‚’æƒ³å®š
    // å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€å¤–éƒ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆPatroniã€pg_auto_failoverãªã©ï¼‰ã‚’ä½¿ç”¨
    
    replica := fm.cluster.replicas[index]
    
    // ãƒ¬ãƒ—ãƒªã‚«ã‚’ãƒã‚¹ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã«æ˜‡æ ¼
    _, err := replica.Exec("SELECT pg_promote()")
    if err != nil {
        return fmt.Errorf("failed to promote replica: %w", err)
    }
    
    // æ–°ã—ã„ãƒã‚¹ã‚¿ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆ
    oldMaster := fm.cluster.master
    fm.cluster.master = replica
    
    // å¤ã„ãƒã‚¹ã‚¿ãƒ¼ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
    if err := oldMaster.Close(); err != nil {
        log.Printf("Failed to close old master connection: %v", err)
    }
    
    // ãƒ¬ãƒ—ãƒªã‚«ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
    fm.cluster.replicas[index] = nil
    delete(fm.cluster.replicaHealth, index)
    
    return nil
}
```

ğŸ“ **èª²é¡Œ**

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤Read-Replicaåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

1. **`DatabaseCluster`**: Master/Replicaæ§‹æˆã®ç®¡ç†
2. **`QueryRouter`**: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚¯ã‚¨ãƒªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
3. **`ConsistencyManager`**: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã®åˆ¶å¾¡
4. **`FailoverManager`**: è‡ªå‹•ãƒ•ã‚§ãƒ¼ãƒ«ã‚ªãƒ¼ãƒãƒ¼æ©Ÿèƒ½
5. **`PerformanceMonitor`**: è² è·åˆ†æ•£åŠ¹æœã®æ¸¬å®š
6. **çµ±åˆãƒ†ã‚¹ãƒˆ**: å®Ÿéš›ã®ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ

âœ… **æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹•**

å®Ÿè£…ãŒå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‹•ä½œãŒæœŸå¾…ã•ã‚Œã¾ã™ï¼š

```bash
$ go test -v
=== RUN   TestDatabaseCluster_BasicRouting
--- PASS: TestDatabaseCluster_BasicRouting (0.05s)
=== RUN   TestQueryRouter_IntelligentRouting
--- PASS: TestQueryRouter_IntelligentRouting (0.08s)
=== RUN   TestConsistencyManager_ReadAfterWrite
--- PASS: TestConsistencyManager_ReadAfterWrite (0.10s)
=== RUN   TestFailoverManager_AutoRecovery
--- PASS: TestFailoverManager_AutoRecovery (0.15s)
=== RUN   TestPerformanceMonitor_LoadDistribution
--- PASS: TestPerformanceMonitor_LoadDistribution (0.12s)
PASS
ok      day40-read-replica    0.500s
```

ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

1. **PostgreSQL Streaming Replication**: WAL-based ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
2. **Connection Pooling**: pgxpool ã‚„ sqlx ã§ã®æ¥ç¶šç®¡ç†
3. **Load Balancing**: Weighted Round Robin ã‚„ Least Connections
4. **Health Checking**: å®šæœŸçš„ãªping ã¨ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ†ã‚¹ãƒˆ
5. **Consistency Models**: CAPå®šç†ã¨å®Ÿç”¨çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š
- **ã‚¯ã‚¨ãƒªåˆ†æ**: æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹èª­ã¿æ›¸ãåˆ¤å®šã®ç²¾åº¦
- **ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶**: PostgreSQLã®LSN ã‚’ä½¿ã£ãŸæ­£ç¢ºãªæ¸¬å®š
- **ãƒ•ã‚§ãƒ¼ãƒ«ã‚ªãƒ¼ãƒãƒ¼**: æ®µéšçš„ãªéšœå®³æ¤œå‡ºã¨è‡ªå‹•å¾©æ—§
- **ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†**: ã‚¹ãƒ†ã‚£ãƒƒã‚­ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ•´åˆæ€§ä¿è¨¼

## å®Ÿè¡Œæ–¹æ³•

```bash
go test -v
go test -race  # ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã®æ¤œå‡º
go test -bench=.  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
```

// DBCluster manages primary and replica databases
type DBCluster struct {
    primary  *sqlx.DB
    replicas []*sqlx.DB
    mu       sync.RWMutex
    current  int // Round-robin counter for replicas
}

func NewDBCluster(primaryDSN string, replicaDSNs []string) (*DBCluster, error) {
    primary, err := sqlx.Open("postgres", primaryDSN)
    if err != nil {
        return nil, err
    }
    
    replicas := make([]*sqlx.DB, len(replicaDSNs))
    for i, dsn := range replicaDSNs {
        replica, err := sqlx.Open("postgres", dsn)
        if err != nil {
            return nil, err
        }
        replicas[i] = replica
    }
    
    return &DBCluster{
        primary:  primary,
        replicas: replicas,
    }, nil
}

// GetPrimary returns the primary database for write operations
func (cluster *DBCluster) GetPrimary() *sqlx.DB {
    return cluster.primary
}

// GetReplica returns a replica database for read operations
func (cluster *DBCluster) GetReplica() *sqlx.DB {
    if len(cluster.replicas) == 0 {
        return cluster.primary // Fallback to primary
    }
    
    cluster.mu.Lock()
    defer cluster.mu.Unlock()
    
    replica := cluster.replicas[cluster.current]
    cluster.current = (cluster.current + 1) % len(cluster.replicas)
    
    return replica
}
```

### æ“ä½œã®åˆ†é›¢

```go
// UserService demonstrates read-write splitting
type UserService struct {
    cluster *DBCluster
}

func NewUserService(cluster *DBCluster) *UserService {
    return &UserService{cluster: cluster}
}

// CreateUser performs write operation on primary
func (us *UserService) CreateUser(ctx context.Context, user *User) error {
    db := us.cluster.GetPrimary()
    
    query := `
        INSERT INTO users (name, email, age) 
        VALUES ($1, $2, $3) 
        RETURNING id, created_at`
    
    return db.GetContext(ctx, user, query, user.Name, user.Email, user.Age)
}

// GetUser performs read operation on replica
func (us *UserService) GetUser(ctx context.Context, id int) (*User, error) {
    db := us.cluster.GetReplica()
    
    var user User
    err := db.GetContext(ctx, &user, "SELECT * FROM users WHERE id = $1", id)
    return &user, err
}

// UpdateUser performs write operation on primary
func (us *UserService) UpdateUser(ctx context.Context, user *User) error {
    db := us.cluster.GetPrimary()
    
    query := `
        UPDATE users 
        SET name = $1, email = $2, age = $3, updated_at = NOW() 
        WHERE id = $4`
    
    _, err := db.ExecContext(ctx, query, user.Name, user.Email, user.Age, user.ID)
    return err
}

// SearchUsers performs read operation on replica
func (us *UserService) SearchUsers(ctx context.Context, filter UserFilter) ([]User, error) {
    db := us.cluster.GetReplica()
    
    // Complex search query that benefits from replica
    query := `
        SELECT * FROM users 
        WHERE ($1 = '' OR name ILIKE '%' || $1 || '%')
          AND ($2 = '' OR email ILIKE '%' || $2 || '%')
          AND ($3 = 0 OR age >= $3)
        ORDER BY created_at DESC
        LIMIT $4 OFFSET $5`
    
    var users []User
    err := db.SelectContext(ctx, &users, query, 
        filter.Name, filter.Email, filter.MinAge, filter.Limit, filter.Offset)
    return users, err
}
```

### é«˜åº¦ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥

```go
// ReadWriteRouter provides intelligent query routing
type ReadWriteRouter struct {
    cluster    *DBCluster
    health     *HealthChecker
    metrics    *RoutingMetrics
    strategy   RoutingStrategy
}

type RoutingStrategy interface {
    SelectReplica(replicas []*sqlx.DB, metrics *RoutingMetrics) *sqlx.DB
}

// RoundRobinStrategy implements round-robin replica selection
type RoundRobinStrategy struct {
    current int
    mu      sync.Mutex
}

func (rr *RoundRobinStrategy) SelectReplica(replicas []*sqlx.DB, metrics *RoutingMetrics) *sqlx.DB {
    if len(replicas) == 0 {
        return nil
    }
    
    rr.mu.Lock()
    defer rr.mu.Unlock()
    
    replica := replicas[rr.current]
    rr.current = (rr.current + 1) % len(replicas)
    
    return replica
}

// WeightedStrategy implements weighted replica selection
type WeightedStrategy struct {
    weights []int
    mu      sync.RWMutex
}

func (ws *WeightedStrategy) SelectReplica(replicas []*sqlx.DB, metrics *RoutingMetrics) *sqlx.DB {
    ws.mu.RLock()
    defer ws.mu.RUnlock()
    
    // Implementation of weighted selection based on replica performance
    totalWeight := 0
    for _, weight := range ws.weights {
        totalWeight += weight
    }
    
    // Simplified weighted selection
    if totalWeight > 0 && len(ws.weights) == len(replicas) {
        return replicas[0] // Simplified for example
    }
    
    return replicas[0]
}
```

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼

```go
// HealthChecker monitors database health
type HealthChecker struct {
    cluster     *DBCluster
    healthMap   map[*sqlx.DB]bool
    mu          sync.RWMutex
    checkInterval time.Duration
}

func NewHealthChecker(cluster *DBCluster, checkInterval time.Duration) *HealthChecker {
    hc := &HealthChecker{
        cluster:       cluster,
        healthMap:     make(map[*sqlx.DB]bool),
        checkInterval: checkInterval,
    }
    
    // Initialize health status
    hc.healthMap[cluster.primary] = true
    for _, replica := range cluster.replicas {
        hc.healthMap[replica] = true
    }
    
    return hc
}

func (hc *HealthChecker) Start() {
    go func() {
        ticker := time.NewTicker(hc.checkInterval)
        defer ticker.Stop()
        
        for range ticker.C {
            hc.checkHealth()
        }
    }()
}

func (hc *HealthChecker) checkHealth() {
    hc.mu.Lock()
    defer hc.mu.Unlock()
    
    // Check primary
    hc.healthMap[hc.cluster.primary] = hc.pingDatabase(hc.cluster.primary)
    
    // Check replicas
    for _, replica := range hc.cluster.replicas {
        hc.healthMap[replica] = hc.pingDatabase(replica)
    }
}

func (hc *HealthChecker) pingDatabase(db *sqlx.DB) bool {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    return db.PingContext(ctx) == nil
}

func (hc *HealthChecker) GetHealthyReplicas() []*sqlx.DB {
    hc.mu.RLock()
    defer hc.mu.RUnlock()
    
    healthy := make([]*sqlx.DB, 0)
    for _, replica := range hc.cluster.replicas {
        if hc.healthMap[replica] {
            healthy = append(healthy, replica)
        }
    }
    
    return healthy
}
```

### ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚°ã®è€ƒæ…®

```go
// ReplicationLagDetector monitors replication lag
type ReplicationLagDetector struct {
    cluster *DBCluster
    maxLag  time.Duration
}

func NewReplicationLagDetector(cluster *DBCluster, maxLag time.Duration) *ReplicationLagDetector {
    return &ReplicationLagDetector{
        cluster: cluster,
        maxLag:  maxLag,
    }
}

func (rld *ReplicationLagDetector) CheckReplicationLag(ctx context.Context) (map[*sqlx.DB]time.Duration, error) {
    lagMap := make(map[*sqlx.DB]time.Duration)
    
    // Get current time from primary
    var primaryTime time.Time
    err := rld.cluster.GetPrimary().GetContext(ctx, &primaryTime, "SELECT NOW()")
    if err != nil {
        return nil, err
    }
    
    // Check lag for each replica
    for _, replica := range rld.cluster.replicas {
        var replicaTime time.Time
        err := replica.GetContext(ctx, &replicaTime, "SELECT NOW()")
        if err != nil {
            lagMap[replica] = time.Hour // Mark as severely lagged
            continue
        }
        
        lag := primaryTime.Sub(replicaTime)
        if lag < 0 {
            lag = 0 // Replica might be ahead due to clock differences
        }
        
        lagMap[replica] = lag
    }
    
    return lagMap, nil
}

func (rld *ReplicationLagDetector) GetLowLagReplicas(ctx context.Context) ([]*sqlx.DB, error) {
    lagMap, err := rld.CheckReplicationLag(ctx)
    if err != nil {
        return nil, err
    }
    
    lowLagReplicas := make([]*sqlx.DB, 0)
    for replica, lag := range lagMap {
        if lag <= rld.maxLag {
            lowLagReplicas = append(lowLagReplicas, replica)
        }
    }
    
    return lowLagReplicas, nil
}
```

### ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†

```go
// TransactionManager handles transactions with read-replica awareness
type TransactionManager struct {
    cluster *DBCluster
}

func NewTransactionManager(cluster *DBCluster) *TransactionManager {
    return &TransactionManager{cluster: cluster}
}

// WithReadOnlyTransaction executes read-only operations in a transaction
func (tm *TransactionManager) WithReadOnlyTransaction(ctx context.Context, fn func(*sqlx.Tx) error) error {
    db := tm.cluster.GetReplica()
    
    tx, err := db.BeginTxx(ctx, &sql.TxOptions{ReadOnly: true})
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    if err := fn(tx); err != nil {
        return err
    }
    
    return tx.Commit()
}

// WithWriteTransaction executes write operations in a transaction
func (tm *TransactionManager) WithWriteTransaction(ctx context.Context, fn func(*sqlx.Tx) error) error {
    db := tm.cluster.GetPrimary()
    
    tx, err := db.BeginTxx(ctx, nil)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    if err := fn(tx); err != nil {
        return err
    }
    
    return tx.Commit()
}
```

ğŸ“ **èª²é¡Œ**

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤Read-Replicaåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

1. **`DBCluster`**: Primary/Replicaã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†
2. **`RoutingManager`**: èª­ã¿æ›¸ãæ“ä½œã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
3. **`HealthMonitor`**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
4. **`LagDetector`**: ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚°ç›£è¦–
5. **`LoadBalancer`**: ãƒ¬ãƒ—ãƒªã‚«é–“ã®è² è·åˆ†æ•£
6. **`FailoverManager`**: è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æ©Ÿèƒ½

âœ… **æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹•**

å®Ÿè£…ãŒå®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‹•ä½œãŒæœŸå¾…ã•ã‚Œã¾ã™ï¼š

```bash
$ go test -v
=== RUN   TestDBCluster_BasicOperations
--- PASS: TestDBCluster_BasicOperations (0.02s)
=== RUN   TestRoutingManager_ReadWriteSplit
--- PASS: TestRoutingManager_ReadWriteSplit (0.05s)
=== RUN   TestHealthMonitor_FailureDetection
--- PASS: TestHealthMonitor_FailureDetection (0.10s)
=== RUN   TestLagDetector_ReplicationMonitoring
--- PASS: TestLagDetector_ReplicationMonitoring (0.08s)
=== RUN   TestLoadBalancer_ReplicaSelection
--- PASS: TestLoadBalancer_ReplicaSelection (0.03s)
PASS
ok      day40-read-replica    0.280s
```

ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**

å®Ÿè£…ã«è©°ã¾ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

1. **Context**: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
2. **sync**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: ä¸¦è¡Œå®‰å…¨ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç®¡ç†
3. **time**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–“éš”ã¨ãƒ©ã‚°æ¸¬å®š
4. **database/sql**: èª­ã¿å–ã‚Šå°‚ç”¨ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
5. **Load balancing**: ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã€é‡ã¿ä»˜ã‘é¸æŠ

Read-Replicaè¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š
- **èª­ã¿æ›¸ãåˆ†é›¢**: æ˜ç¢ºãªæ“ä½œã®åˆ†é¡
- **ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼**: Primaryéšœå®³æ™‚ã®ãƒ¬ãƒ—ãƒªã‚«æ˜‡æ ¼
- **ãƒ©ã‚°ç›£è¦–**: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç®¡ç†
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: éšœå®³ã®æ—©æœŸæ¤œå‡º

## å®Ÿè¡Œæ–¹æ³•

```bash
go test -v
go test -race  # ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã®æ¤œå‡º
go test -bench=.  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
```
# Day 44: Cache Invalidation Strategies

## ğŸ¯ æœ¬æ—¥ã®ç›®æ¨™ (Today's Goal)

æ§˜ã€…ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã€ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ä¿ã¡ãªãŒã‚‰åŠ¹ç‡çš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’è¡Œãˆã‚‹æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹ã€‚TTLã€ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ç„¡åŠ¹åŒ–ã€ä¾å­˜é–¢ä¿‚ç®¡ç†ãªã©ã®é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹ã€‚

## ğŸ“– è§£èª¬ (Explanation)

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ã®é‡è¦æ€§

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯æ€§èƒ½å‘ä¸Šã®ãŸã‚ã«ä¸å¯æ¬ ã§ã™ãŒã€å¤ã„ãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚Šç¶šã‘ã‚‹ã¨ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ãŒæãªã‚ã‚Œã¾ã™ã€‚åŠ¹æœçš„ãªç„¡åŠ¹åŒ–æˆ¦ç•¥ã«ã‚ˆã‚Šã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨æ•´åˆæ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã¾ã™ã€‚

### ä¸»ãªç„¡åŠ¹åŒ–æˆ¦ç•¥

#### 1. TTL (Time To Live) ãƒ™ãƒ¼ã‚¹
- æ™‚é–“çµŒéã«ã‚ˆã‚‹è‡ªå‹•ç„¡åŠ¹åŒ–
- è¨­å®šãŒç°¡å˜ã§äºˆæ¸¬å¯èƒ½
- ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°é »åº¦ã«åŸºã¥ãèª¿æ•´ãŒé‡è¦

#### 2. ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ç„¡åŠ¹åŒ–
- ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ™‚ã®å³åº§ãªç„¡åŠ¹åŒ–
- é«˜ã„æ•´åˆæ€§ã‚’ä¿è¨¼
- è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ã®ç®¡ç†ãŒå¿…è¦

#### 3. ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ç„¡åŠ¹åŒ–
- é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ä¸€æ‹¬ç„¡åŠ¹åŒ–
- æŸ”è»Ÿãªç„¡åŠ¹åŒ–ãƒãƒªã‚·ãƒ¼
- Redis Sets ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªå®Ÿè£…

#### 4. ä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ç„¡åŠ¹åŒ–
- ãƒ‡ãƒ¼ã‚¿é–“ã®ä¾å­˜é–¢ä¿‚ã‚’å®šç¾©
- é€£é–çš„ãªç„¡åŠ¹åŒ–å‡¦ç†
- ã‚°ãƒ©ãƒ•ç†è«–ã‚’æ´»ç”¨ã—ãŸæœ€é©åŒ–

## ğŸ“ èª²é¡Œ (The Problem)

ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

### 1. CacheInvalidator ã®å®Ÿè£…

```go
type CacheInvalidator struct {
    cache     CacheClient
    tagStore  TagStore
    ruleEngine RuleEngine
    metrics   *InvalidationMetrics
}
```

### 2. å¿…è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…

- `InvalidateByKey(ctx context.Context, key string) error`: å€‹åˆ¥ã‚­ãƒ¼ç„¡åŠ¹åŒ–
- `InvalidateByTag(ctx context.Context, tag string) error`: ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ç„¡åŠ¹åŒ–
- `InvalidateByPattern(ctx context.Context, pattern string) error`: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒç„¡åŠ¹åŒ–
- `InvalidateRelated(ctx context.Context, key string) error`: é–¢é€£ãƒ‡ãƒ¼ã‚¿ç„¡åŠ¹åŒ–
- `SetTTL(ctx context.Context, key string, ttl time.Duration) error`: TTLæ›´æ–°
- `AddInvalidationRule(rule InvalidationRule) error`: ç„¡åŠ¹åŒ–ãƒ«ãƒ¼ãƒ«è¿½åŠ 

### 3. é«˜åº¦ãªæ©Ÿèƒ½

- ç„¡åŠ¹åŒ–ã®é…å»¶å®Ÿè¡Œã¨ãƒãƒƒãƒå‡¦ç†
- ç„¡åŠ¹åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›£è¦–
- å¾ªç’°ä¾å­˜ã®æ¤œå‡ºã¨å›é¿
- ç„¡åŠ¹åŒ–å¤±æ•—æ™‚ã®å†è©¦è¡Œæ©Ÿèƒ½

## âœ… æœŸå¾…ã•ã‚Œã‚‹æŒ™å‹• (Expected Behavior)

```bash
$ go test -v
=== RUN   TestCacheInvalidation_TagBased
    main_test.go:85: Tagged cache invalidation successful
    main_test.go:92: All related items invalidated: 15
--- PASS: TestCacheInvalidation_TagBased (0.03s)

=== RUN   TestCacheInvalidation_DependencyChain
    main_test.go:125: Dependency chain invalidation completed
    main_test.go:132: Cascaded invalidation affected 8 keys
--- PASS: TestCacheInvalidation_DependencyChain (0.02s)
```

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ (Hints)

### åŸºæœ¬æ§‹é€ 

```go
type InvalidationRule struct {
    Trigger   string        // ãƒˆãƒªã‚¬ãƒ¼ã¨ãªã‚‹ã‚­ãƒ¼
    Targets   []string      // ç„¡åŠ¹åŒ–å¯¾è±¡ã®ã‚­ãƒ¼/ãƒ‘ã‚¿ãƒ¼ãƒ³
    Delay     time.Duration // é…å»¶æ™‚é–“
    Condition func() bool   // å®Ÿè¡Œæ¡ä»¶
}

type TagStore interface {
    AddTag(ctx context.Context, key, tag string) error
    GetKeysByTag(ctx context.Context, tag string) ([]string, error)
    RemoveTag(ctx context.Context, key, tag string) error
}
```

### Redis Lua ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚‹åŠ¹ç‡åŒ–

```lua
-- ã‚¿ã‚°ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ã‚­ãƒ¼ã‚’ä¸€æ‹¬å‰Šé™¤
local tag = ARGV[1]
local keys = redis.call('SMEMBERS', 'tag:' .. tag)
for i=1,#keys do
    redis.call('DEL', keys[i])
end
redis.call('DEL', 'tag:' .. tag)
return #keys
```

## ğŸš¨ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç’°å¢ƒã§ã®å®Ÿç½å®³ã‚·ãƒŠãƒªã‚ªã¨å¯¾ç­–

### å®Ÿéš›ã®éšœå®³äº‹ä¾‹ã¨ãƒªã‚«ãƒãƒªãƒ¼æˆ¦ç•¥

#### âŒ ç½å®³äº‹ä¾‹1: Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼éƒ¨åˆ†éšœå®³ã«ã‚ˆã‚‹ç„¡åŠ¹åŒ–å¤±æ•—

**ç™ºç”ŸçŠ¶æ³:** 
- å¤§æ‰‹ECã‚µã‚¤ãƒˆã§Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®1ãƒãƒ¼ãƒ‰ãŒéšœå®³ã§ãƒ€ã‚¦ãƒ³
- è©²å½“ã‚·ãƒ£ãƒ¼ãƒ‰ã®å•†å“ä¾¡æ ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹åŒ–ã§ããš
- ã‚»ãƒ¼ãƒ«ä¾¡æ ¼æ›´æ–°ãŒåæ˜ ã•ã‚Œãšã«æ­£è¦ä¾¡æ ¼ã§è²©å£²ç¶™ç¶š

**æŠ€è¡“çš„ãªå•é¡Œ:**
```go
// âŒ å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹
func (invalidator *SimpleInvalidator) InvalidatePrice(productID string) error {
    key := fmt.Sprintf("price:%s", productID)
    // å˜ä¸€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã¿ã«ä¾å­˜ - éšœå®³æ™‚ã«å¤±æ•—
    return invalidator.redisClient.Del(key).Err()
}
```

**ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿:**
- æ¨å®šæå¤±: å£²ä¸Š2,000ä¸‡å††ã®æ©Ÿä¼šæå¤±
- é¡§å®¢æº€è¶³åº¦ä½ä¸‹: ä¾¡æ ¼ä¸æ•´åˆã«ã‚ˆã‚‹ã‚¯ãƒ¬ãƒ¼ãƒ 500ä»¶
- ã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ä½ä¸‹: SLAé•åã«ã‚ˆã‚‹å¥‘ç´„å•é¡Œ

âœ… **ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®å†—é•·åŒ–å¯¾ç­–:**

```go
type ResilientInvalidator struct {
    primaryCluster   *redis.ClusterClient
    fallbackCluster  *redis.ClusterClient
    backupQueue      *InvalidationQueue
    alertManager     *AlertManager
    metrics         *InvalidationMetrics
}

func (r *ResilientInvalidator) InvalidateWithFailover(
    ctx context.Context, key string) error {
    
    var errors []error
    
    // Primary cluster ã¸ã®ç„¡åŠ¹åŒ–è©¦è¡Œ
    if err := r.primaryCluster.Del(ctx, key).Err(); err != nil {
        r.metrics.PrimaryFailures.Inc()
        errors = append(errors, fmt.Errorf("primary cluster failed: %w", err))
        
        // å³åº§ã«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        r.alertManager.SendAlert(AlertLevel.Warning, 
            fmt.Sprintf("Primary cache invalidation failed for key: %s", key))
    }
    
    // Fallback cluster ã¸ã®ç„¡åŠ¹åŒ–è©¦è¡Œ
    if err := r.fallbackCluster.Del(ctx, key).Err(); err != nil {
        r.metrics.FallbackFailures.Inc()
        errors = append(errors, fmt.Errorf("fallback cluster failed: %w", err))
    }
    
    // ä¸¡æ–¹å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚­ãƒ¥ãƒ¼ã«ä¿å­˜
    if len(errors) == 2 {
        r.backupQueue.Enqueue(InvalidationTask{
            Key:        key,
            Timestamp:  time.Now(),
            RetryCount: 0,
            Priority:   High,
        })
        
        r.alertManager.SendAlert(AlertLevel.Critical,
            fmt.Sprintf("ALL cache invalidation failed for key: %s", key))
    }
    
    return combineErrors(errors)
}

// ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
func (r *ResilientInvalidator) startRetryWorker(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            r.processRetryQueue(ctx)
        }
    }
}

func (r *ResilientInvalidator) processRetryQueue(ctx context.Context) {
    tasks := r.backupQueue.DequeueAll()
    
    for _, task := range tasks {
        if task.RetryCount >= 3 {
            // 3å›å¤±æ•—ã—ãŸã‚‰Dead Letter Queueã¸
            r.backupQueue.MoveToDLQ(task)
            continue
        }
        
        if err := r.InvalidateWithFailover(ctx, task.Key); err != nil {
            task.RetryCount++
            task.NextRetry = time.Now().Add(
                time.Duration(task.RetryCount) * time.Minute)
            r.backupQueue.Enqueue(task)
        }
    }
}
```

#### âŒ ç½å®³äº‹ä¾‹2: å¤§è¦æ¨¡ãƒãƒƒãƒç„¡åŠ¹åŒ–ã«ã‚ˆã‚‹æ€§èƒ½åŠ£åŒ–

**ç™ºç”ŸçŠ¶æ³:**
- ã‚»ãƒ¼ãƒ«é–‹å§‹æ™‚ã«100ä¸‡å€‹ã®å•†å“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¸€æ‹¬ç„¡åŠ¹åŒ–
- ç„¡åŠ¹åŒ–å‡¦ç†ã«5åˆ†ã‹ã‹ã‚Šã€å¤ã„ä¾¡æ ¼æƒ…å ±ãŒæ®‹å­˜
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®å¤§é‡ã‚¢ã‚¯ã‚»ã‚¹ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒ30ç§’ã«åŠ£åŒ–

**å•é¡Œã®åˆ†æ:**
```go
// âŒ å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ - é€æ¬¡å‡¦ç†ã§é…ã„
func (invalidator *NaiveInvalidator) InvalidateBatch(keys []string) error {
    for _, key := range keys {  // 100ä¸‡å›ã®ãƒ«ãƒ¼ãƒ—
        if err := invalidator.client.Del(key).Err(); err != nil {
            return err  // 1ã¤å¤±æ•—ã™ã‚‹ã¨å…¨ä½“ãŒæ­¢ã¾ã‚‹
        }
        time.Sleep(1 * time.Millisecond)  // éåº¦ãªé…æ…®ã§é…å»¶
    }
    return nil
}
```

âœ… **é«˜æ€§èƒ½ãƒãƒƒãƒç„¡åŠ¹åŒ–ã‚·ã‚¹ãƒ†ãƒ :**

```go
type HighPerformanceBatchInvalidator struct {
    workerCount     int
    batchSize       int
    rateLimiter     *rate.Limiter
    clients         []*redis.Client  // è¤‡æ•°ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³
    metrics        *BatchMetrics
}

func (b *HighPerformanceBatchInvalidator) InvalidateMassively(
    ctx context.Context, keys []string) error {
    
    start := time.Now()
    defer func() {
        b.metrics.BatchDuration.Observe(time.Since(start).Seconds())
        b.metrics.BatchSize.Observe(float64(len(keys)))
    }()
    
    // ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    chunks := b.chunkKeys(keys, b.batchSize)
    jobs := make(chan []string, len(chunks))
    results := make(chan BatchResult, len(chunks))
    
    // ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«èµ·å‹•
    for i := 0; i < b.workerCount; i++ {
        go b.batchWorker(ctx, i, jobs, results)
    }
    
    // ã‚¸ãƒ§ãƒ–æŠ•å…¥
    for _, chunk := range chunks {
        select {
        case jobs <- chunk:
        case <-ctx.Done():
            return ctx.Err()
        }
    }
    close(jobs)
    
    // çµæœé›†ç´„
    var totalErrors []error
    successCount := 0
    
    for i := 0; i < len(chunks); i++ {
        select {
        case result := <-results:
            if result.Error != nil {
                totalErrors = append(totalErrors, result.Error)
            } else {
                successCount += result.ProcessedCount
            }
        case <-ctx.Done():
            return ctx.Err()
        }
    }
    
    b.metrics.SuccessfulInvalidations.Add(float64(successCount))
    b.metrics.FailedInvalidations.Add(float64(len(totalErrors)))
    
    if len(totalErrors) > 0 {
        return fmt.Errorf("batch invalidation completed with %d errors: %v", 
            len(totalErrors), totalErrors[:min(5, len(totalErrors))])
    }
    
    return nil
}

func (b *HighPerformanceBatchInvalidator) batchWorker(
    ctx context.Context, workerID int, jobs <-chan []string, results chan<- BatchResult) {
    
    client := b.clients[workerID%len(b.clients)]  // è² è·åˆ†æ•£
    
    for chunk := range jobs {
        // ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
        if err := b.rateLimiter.Wait(ctx); err != nil {
            results <- BatchResult{Error: err}
            continue
        }
        
        // Pipeline ã§ã¾ã¨ã‚ã¦å®Ÿè¡Œ
        pipe := client.Pipeline()
        for _, key := range chunk {
            pipe.Del(ctx, key)
        }
        
        cmds, err := pipe.Exec(ctx)
        if err != nil {
            results <- BatchResult{Error: err}
            continue
        }
        
        // å€‹åˆ¥ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
        failedCount := 0
        for _, cmd := range cmds {
            if cmd.Err() != nil {
                failedCount++
            }
        }
        
        results <- BatchResult{
            ProcessedCount: len(chunk) - failedCount,
            FailedCount:   failedCount,
        }
    }
}

type BatchResult struct {
    ProcessedCount int
    FailedCount    int
    Error         error
}
```

### ğŸ“Š ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºé‹ç”¨ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

#### Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©

```go
type InvalidationMetrics struct {
    InvalidationRate      *prometheus.GaugeVec
    InvalidationLatency   *prometheus.HistogramVec
    FailedInvalidations   *prometheus.CounterVec
    CascadeDepth         *prometheus.HistogramVec
    QueueSize            *prometheus.GaugeVec
}

func NewInvalidationMetrics() *InvalidationMetrics {
    return &InvalidationMetrics{
        InvalidationRate: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "cache_invalidations_per_second",
                Help: "Cache invalidations per second by type and source",
            },
            []string{"type", "source", "cluster"},
        ),
        
        InvalidationLatency: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "cache_invalidation_duration_seconds",
                Help: "Time taken to invalidate cache entries",
                Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5},
            },
            []string{"method", "result", "cluster"},
        ),
        
        FailedInvalidations: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "cache_invalidation_failures_total",
                Help: "Total number of failed cache invalidations",
            },
            []string{"error_type", "cluster", "key_pattern"},
        ),
        
        CascadeDepth: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "cache_invalidation_cascade_depth",
                Help: "Depth of cascading invalidations",
                Buckets: []float64{1, 2, 3, 5, 10, 20, 50},
            },
            []string{"trigger_type"},
        ),
    }
}
```

#### é‹ç”¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šä¾‹ï¼ˆAlertManagerï¼‰

```yaml
groups:
- name: cache-invalidation-critical
  rules:
  - alert: CacheInvalidationRateCritical
    expr: rate(cache_invalidations_per_second[5m]) > 10000
    for: 1m
    labels:
      severity: critical
      team: backend
      runbook: "https://wiki.company.com/runbooks/cache-storm"
    annotations:
      summary: "Cache invalidation rate critically high"
      description: "Invalidation rate is {{ $value }} per second (threshold: 10000)"
      impact: "Potential cache storm affecting system performance"
      
  - alert: CacheInvalidationCascadeTooDeep
    expr: histogram_quantile(0.95, cache_invalidation_cascade_depth) > 10
    for: 2m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "Cache invalidation cascade too deep"
      description: "95th percentile cascade depth is {{ $value }} (threshold: 10)"
      
  - alert: CacheInvalidationFailureRateHigh
    expr: rate(cache_invalidation_failures_total[5m]) / rate(cache_invalidations_per_second[5m]) > 0.1
    for: 3m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "High cache invalidation failure rate"
      description: "Invalidation failure rate is {{ $value | humanizePercentage }}"

- name: cache-invalidation-capacity
  rules:
  - alert: InvalidationQueueBacklog
    expr: cache_invalidation_queue_size > 100000
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "Cache invalidation queue backlog growing"
      description: "Queue size: {{ $value }} items"
```

#### Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šä¾‹

```json
{
  "dashboard": {
    "title": "Cache Invalidation Monitoring",
    "panels": [
      {
        "title": "Invalidation Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(cache_invalidations_per_second[5m])",
            "legendFormat": "{{type}} - {{source}}"
          }
        ],
        "alert": {
          "conditions": [
            {
              "query": {"queryType": "", "refId": "A"},
              "reducer": {"type": "last", "params": []},
              "evaluator": {"params": [5000], "type": "gt"}
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "10s",
          "handler": 1,
          "name": "Cache Invalidation Rate Alert"
        }
      }
    ]
  }
}
```

## ğŸš€ ç™ºå±•èª²é¡Œ

1. **éšå±¤çš„ã‚¿ã‚°ã‚·ã‚¹ãƒ†ãƒ **: ãƒã‚¹ãƒˆã—ãŸã‚¿ã‚°ã«ã‚ˆã‚‹ç´°ã‹ã„åˆ¶å¾¡
2. **ç„¡åŠ¹åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°**: cron ã®ã‚ˆã†ãªå®šæœŸå®Ÿè¡Œ
3. **åˆ†æ•£ç„¡åŠ¹åŒ–**: ãƒãƒ«ãƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç’°å¢ƒã§ã®åŒæœŸ
4. **ç„¡åŠ¹åŒ–ç›£æŸ»**: ç„¡åŠ¹åŒ–æ“ä½œã®å®Œå…¨ãªãƒ­ã‚°è¨˜éŒ²
5. **äºˆæ¸¬çš„ç„¡åŠ¹åŒ–**: ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«åŸºã¥ãæœ€é©åŒ–
6. **åœ°ç†çš„åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: è¤‡æ•°ãƒªãƒ¼ã‚¸ãƒ§ãƒ³é–“ã§ã®ç„¡åŠ¹åŒ–åŒæœŸ

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–æˆ¦ç•¥ã®å®Ÿè£…ã‚’é€šã˜ã¦ã€å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã§ã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç®¡ç†æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã—ã‚‡ã†ï¼